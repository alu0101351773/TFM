{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importación de librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lectura de fichero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Volumen dep. almacenam. ini. (L)</th>\n",
       "      <th>Venta (L)</th>\n",
       "      <th>Llenado dep. almacenam. (L)</th>\n",
       "      <th>Volumen dep. almacenam. fin. teor. (L)</th>\n",
       "      <th>Volumen dep. almacenam. fin. (L)</th>\n",
       "      <th>Variacion</th>\n",
       "      <th>Variacion Acum.</th>\n",
       "      <th>Fugando combustible</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20300,59</td>\n",
       "      <td>-5785</td>\n",
       "      <td>7017,89</td>\n",
       "      <td>21533,48</td>\n",
       "      <td>21522,72</td>\n",
       "      <td>-10,76</td>\n",
       "      <td>234,6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21522,72</td>\n",
       "      <td>-6040</td>\n",
       "      <td>7983,27</td>\n",
       "      <td>23465,99</td>\n",
       "      <td>23302,24</td>\n",
       "      <td>-163,75</td>\n",
       "      <td>35,97</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23302,24</td>\n",
       "      <td>-6005</td>\n",
       "      <td>4558,82</td>\n",
       "      <td>21856,06</td>\n",
       "      <td>21823,0</td>\n",
       "      <td>-33,06</td>\n",
       "      <td>-38,51</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21823,0</td>\n",
       "      <td>-5820</td>\n",
       "      <td>5284,44</td>\n",
       "      <td>21287,44</td>\n",
       "      <td>21257,0</td>\n",
       "      <td>-30,44</td>\n",
       "      <td>-45,57</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21257,0</td>\n",
       "      <td>-5975</td>\n",
       "      <td>6238,65</td>\n",
       "      <td>21520,65</td>\n",
       "      <td>21639,32</td>\n",
       "      <td>118,67</td>\n",
       "      <td>-4,62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>21639,32</td>\n",
       "      <td>-5895</td>\n",
       "      <td>7578,84</td>\n",
       "      <td>23323,16</td>\n",
       "      <td>23420,13</td>\n",
       "      <td>96,97</td>\n",
       "      <td>105,45</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>23420,13</td>\n",
       "      <td>-5820</td>\n",
       "      <td>4391,04</td>\n",
       "      <td>21991,17</td>\n",
       "      <td>22048,16</td>\n",
       "      <td>56,99</td>\n",
       "      <td>158,48</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>22048,16</td>\n",
       "      <td>-6135</td>\n",
       "      <td>7710,2</td>\n",
       "      <td>23623,36</td>\n",
       "      <td>23568,83</td>\n",
       "      <td>-54,53</td>\n",
       "      <td>48,56</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>23568,83</td>\n",
       "      <td>-5700</td>\n",
       "      <td>3708,58</td>\n",
       "      <td>21577,41</td>\n",
       "      <td>21671,39</td>\n",
       "      <td>93,98</td>\n",
       "      <td>141,14</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>21671,39</td>\n",
       "      <td>-6240</td>\n",
       "      <td>6945,83</td>\n",
       "      <td>22377,22</td>\n",
       "      <td>22417,02</td>\n",
       "      <td>39,8</td>\n",
       "      <td>113,87</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>22417,02</td>\n",
       "      <td>-6635</td>\n",
       "      <td>8916,91</td>\n",
       "      <td>24698,93</td>\n",
       "      <td>24664,59</td>\n",
       "      <td>-34,34</td>\n",
       "      <td>90,29</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>24664,59</td>\n",
       "      <td>-5835</td>\n",
       "      <td>3471,0</td>\n",
       "      <td>22300,59</td>\n",
       "      <td>22355,09</td>\n",
       "      <td>54,5</td>\n",
       "      <td>308,54</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>22355,09</td>\n",
       "      <td>-5905</td>\n",
       "      <td>5214,81</td>\n",
       "      <td>21664,9</td>\n",
       "      <td>21648,19</td>\n",
       "      <td>-16,71</td>\n",
       "      <td>324,89</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>21648,19</td>\n",
       "      <td>-5865</td>\n",
       "      <td>5636,35</td>\n",
       "      <td>21419,54</td>\n",
       "      <td>21467,99</td>\n",
       "      <td>48,45</td>\n",
       "      <td>403,78</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>21467,99</td>\n",
       "      <td>-6170</td>\n",
       "      <td>4948,56</td>\n",
       "      <td>20246,55</td>\n",
       "      <td>20227,25</td>\n",
       "      <td>-19,3</td>\n",
       "      <td>265,81</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>20227,25</td>\n",
       "      <td>-5945</td>\n",
       "      <td>7956,88</td>\n",
       "      <td>22239,13</td>\n",
       "      <td>22267,31</td>\n",
       "      <td>28,18</td>\n",
       "      <td>197,02</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>22267,31</td>\n",
       "      <td>-5780</td>\n",
       "      <td>5977,77</td>\n",
       "      <td>22465,08</td>\n",
       "      <td>22511,93</td>\n",
       "      <td>46,85</td>\n",
       "      <td>186,88</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>22511,93</td>\n",
       "      <td>-5895</td>\n",
       "      <td>5989,0</td>\n",
       "      <td>22605,93</td>\n",
       "      <td>22574,78</td>\n",
       "      <td>-31,15</td>\n",
       "      <td>210,26</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>22574,78</td>\n",
       "      <td>-5760</td>\n",
       "      <td>6292,65</td>\n",
       "      <td>23107,43</td>\n",
       "      <td>23200,34</td>\n",
       "      <td>92,91</td>\n",
       "      <td>209,19</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>23200,34</td>\n",
       "      <td>-5935</td>\n",
       "      <td>3753,77</td>\n",
       "      <td>21019,11</td>\n",
       "      <td>20963,77</td>\n",
       "      <td>-55,34</td>\n",
       "      <td>114,05</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20963,77</td>\n",
       "      <td>-5490</td>\n",
       "      <td>6918,55</td>\n",
       "      <td>22392,32</td>\n",
       "      <td>22352,14</td>\n",
       "      <td>-40,18</td>\n",
       "      <td>108,21</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22352,14</td>\n",
       "      <td>-5670</td>\n",
       "      <td>6563,68</td>\n",
       "      <td>23245,82</td>\n",
       "      <td>23206,49</td>\n",
       "      <td>-39,33</td>\n",
       "      <td>14,38</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23206,49</td>\n",
       "      <td>-5805</td>\n",
       "      <td>5117,12</td>\n",
       "      <td>22518,61</td>\n",
       "      <td>22516,41</td>\n",
       "      <td>-2,2</td>\n",
       "      <td>28,89</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>22516,41</td>\n",
       "      <td>-6315</td>\n",
       "      <td>8234,24</td>\n",
       "      <td>24435,65</td>\n",
       "      <td>24424,35</td>\n",
       "      <td>-11,3</td>\n",
       "      <td>-30,86</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24424,35</td>\n",
       "      <td>-5860</td>\n",
       "      <td>3230,48</td>\n",
       "      <td>21794,83</td>\n",
       "      <td>21783,77</td>\n",
       "      <td>-11,06</td>\n",
       "      <td>-22,62</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>21783,77</td>\n",
       "      <td>-6315</td>\n",
       "      <td>5967,93</td>\n",
       "      <td>21436,7</td>\n",
       "      <td>21343,34</td>\n",
       "      <td>-93,36</td>\n",
       "      <td>-144,16</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>21343,34</td>\n",
       "      <td>-6070</td>\n",
       "      <td>5946,98</td>\n",
       "      <td>21220,32</td>\n",
       "      <td>21269,16</td>\n",
       "      <td>48,84</td>\n",
       "      <td>-142,17</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>21269,16</td>\n",
       "      <td>-5900</td>\n",
       "      <td>6411,31</td>\n",
       "      <td>21780,47</td>\n",
       "      <td>21628,04</td>\n",
       "      <td>-152,43</td>\n",
       "      <td>-263,45</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Volumen dep. almacenam. ini. (L)  Venta (L) Llenado dep. almacenam. (L)  \\\n",
       "0                          20300,59      -5785                     7017,89   \n",
       "1                          21522,72      -6040                     7983,27   \n",
       "2                          23302,24      -6005                     4558,82   \n",
       "3                           21823,0      -5820                     5284,44   \n",
       "4                           21257,0      -5975                     6238,65   \n",
       "5                          21639,32      -5895                     7578,84   \n",
       "6                          23420,13      -5820                     4391,04   \n",
       "7                          22048,16      -6135                      7710,2   \n",
       "8                          23568,83      -5700                     3708,58   \n",
       "9                          21671,39      -6240                     6945,83   \n",
       "10                         22417,02      -6635                     8916,91   \n",
       "11                         24664,59      -5835                      3471,0   \n",
       "12                         22355,09      -5905                     5214,81   \n",
       "13                         21648,19      -5865                     5636,35   \n",
       "14                         21467,99      -6170                     4948,56   \n",
       "15                         20227,25      -5945                     7956,88   \n",
       "16                         22267,31      -5780                     5977,77   \n",
       "17                         22511,93      -5895                      5989,0   \n",
       "18                         22574,78      -5760                     6292,65   \n",
       "19                         23200,34      -5935                     3753,77   \n",
       "20                         20963,77      -5490                     6918,55   \n",
       "21                         22352,14      -5670                     6563,68   \n",
       "22                         23206,49      -5805                     5117,12   \n",
       "23                         22516,41      -6315                     8234,24   \n",
       "24                         24424,35      -5860                     3230,48   \n",
       "25                         21783,77      -6315                     5967,93   \n",
       "26                         21343,34      -6070                     5946,98   \n",
       "27                         21269,16      -5900                     6411,31   \n",
       "\n",
       "   Volumen dep. almacenam. fin. teor. (L) Volumen dep. almacenam. fin. (L)  \\\n",
       "0                                21533,48                         21522,72   \n",
       "1                                23465,99                         23302,24   \n",
       "2                                21856,06                          21823,0   \n",
       "3                                21287,44                          21257,0   \n",
       "4                                21520,65                         21639,32   \n",
       "5                                23323,16                         23420,13   \n",
       "6                                21991,17                         22048,16   \n",
       "7                                23623,36                         23568,83   \n",
       "8                                21577,41                         21671,39   \n",
       "9                                22377,22                         22417,02   \n",
       "10                               24698,93                         24664,59   \n",
       "11                               22300,59                         22355,09   \n",
       "12                                21664,9                         21648,19   \n",
       "13                               21419,54                         21467,99   \n",
       "14                               20246,55                         20227,25   \n",
       "15                               22239,13                         22267,31   \n",
       "16                               22465,08                         22511,93   \n",
       "17                               22605,93                         22574,78   \n",
       "18                               23107,43                         23200,34   \n",
       "19                               21019,11                         20963,77   \n",
       "20                               22392,32                         22352,14   \n",
       "21                               23245,82                         23206,49   \n",
       "22                               22518,61                         22516,41   \n",
       "23                               24435,65                         24424,35   \n",
       "24                               21794,83                         21783,77   \n",
       "25                                21436,7                         21343,34   \n",
       "26                               21220,32                         21269,16   \n",
       "27                               21780,47                         21628,04   \n",
       "\n",
       "   Variacion Variacion Acum.  Fugando combustible  \n",
       "0     -10,76           234,6                    0  \n",
       "1    -163,75           35,97                    0  \n",
       "2     -33,06          -38,51                    0  \n",
       "3     -30,44          -45,57                    0  \n",
       "4     118,67           -4,62                    0  \n",
       "5      96,97          105,45                    0  \n",
       "6      56,99          158,48                    0  \n",
       "7     -54,53           48,56                    0  \n",
       "8      93,98          141,14                    0  \n",
       "9       39,8          113,87                    0  \n",
       "10    -34,34           90,29                    0  \n",
       "11      54,5          308,54                    0  \n",
       "12    -16,71          324,89                    0  \n",
       "13     48,45          403,78                    0  \n",
       "14     -19,3          265,81                    1  \n",
       "15     28,18          197,02                    1  \n",
       "16     46,85          186,88                    1  \n",
       "17    -31,15          210,26                    1  \n",
       "18     92,91          209,19                    1  \n",
       "19    -55,34          114,05                    1  \n",
       "20    -40,18          108,21                    1  \n",
       "21    -39,33           14,38                    1  \n",
       "22      -2,2           28,89                    1  \n",
       "23     -11,3          -30,86                    1  \n",
       "24    -11,06          -22,62                    1  \n",
       "25    -93,36         -144,16                    1  \n",
       "26     48,84         -142,17                    1  \n",
       "27   -152,43         -263,45                    1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.read_csv('../../data/datos_simulacion.csv').drop(columns=[\n",
    "    'Unnamed: 0',\n",
    "    'Tiempo (dia)'\n",
    "])\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limpieza de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Volumen dep. almacenam. ini. (L)</th>\n",
       "      <th>Venta (L)</th>\n",
       "      <th>Llenado dep. almacenam. (L)</th>\n",
       "      <th>Volumen dep. almacenam. fin. teor. (L)</th>\n",
       "      <th>Volumen dep. almacenam. fin. (L)</th>\n",
       "      <th>Variacion</th>\n",
       "      <th>Variacion Acum.</th>\n",
       "      <th>Fugando combustible</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20300.59</td>\n",
       "      <td>-5785</td>\n",
       "      <td>7017.89</td>\n",
       "      <td>21533.48</td>\n",
       "      <td>21522.72</td>\n",
       "      <td>-10.76</td>\n",
       "      <td>234.60</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21522.72</td>\n",
       "      <td>-6040</td>\n",
       "      <td>7983.27</td>\n",
       "      <td>23465.99</td>\n",
       "      <td>23302.24</td>\n",
       "      <td>-163.75</td>\n",
       "      <td>35.97</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23302.24</td>\n",
       "      <td>-6005</td>\n",
       "      <td>4558.82</td>\n",
       "      <td>21856.06</td>\n",
       "      <td>21823.00</td>\n",
       "      <td>-33.06</td>\n",
       "      <td>-38.51</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21823.00</td>\n",
       "      <td>-5820</td>\n",
       "      <td>5284.44</td>\n",
       "      <td>21287.44</td>\n",
       "      <td>21257.00</td>\n",
       "      <td>-30.44</td>\n",
       "      <td>-45.57</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21257.00</td>\n",
       "      <td>-5975</td>\n",
       "      <td>6238.65</td>\n",
       "      <td>21520.65</td>\n",
       "      <td>21639.32</td>\n",
       "      <td>118.67</td>\n",
       "      <td>-4.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>21639.32</td>\n",
       "      <td>-5895</td>\n",
       "      <td>7578.84</td>\n",
       "      <td>23323.16</td>\n",
       "      <td>23420.13</td>\n",
       "      <td>96.97</td>\n",
       "      <td>105.45</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>23420.13</td>\n",
       "      <td>-5820</td>\n",
       "      <td>4391.04</td>\n",
       "      <td>21991.17</td>\n",
       "      <td>22048.16</td>\n",
       "      <td>56.99</td>\n",
       "      <td>158.48</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>22048.16</td>\n",
       "      <td>-6135</td>\n",
       "      <td>7710.20</td>\n",
       "      <td>23623.36</td>\n",
       "      <td>23568.83</td>\n",
       "      <td>-54.53</td>\n",
       "      <td>48.56</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>23568.83</td>\n",
       "      <td>-5700</td>\n",
       "      <td>3708.58</td>\n",
       "      <td>21577.41</td>\n",
       "      <td>21671.39</td>\n",
       "      <td>93.98</td>\n",
       "      <td>141.14</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>21671.39</td>\n",
       "      <td>-6240</td>\n",
       "      <td>6945.83</td>\n",
       "      <td>22377.22</td>\n",
       "      <td>22417.02</td>\n",
       "      <td>39.80</td>\n",
       "      <td>113.87</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>22417.02</td>\n",
       "      <td>-6635</td>\n",
       "      <td>8916.91</td>\n",
       "      <td>24698.93</td>\n",
       "      <td>24664.59</td>\n",
       "      <td>-34.34</td>\n",
       "      <td>90.29</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>24664.59</td>\n",
       "      <td>-5835</td>\n",
       "      <td>3471.00</td>\n",
       "      <td>22300.59</td>\n",
       "      <td>22355.09</td>\n",
       "      <td>54.50</td>\n",
       "      <td>308.54</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>22355.09</td>\n",
       "      <td>-5905</td>\n",
       "      <td>5214.81</td>\n",
       "      <td>21664.90</td>\n",
       "      <td>21648.19</td>\n",
       "      <td>-16.71</td>\n",
       "      <td>324.89</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>21648.19</td>\n",
       "      <td>-5865</td>\n",
       "      <td>5636.35</td>\n",
       "      <td>21419.54</td>\n",
       "      <td>21467.99</td>\n",
       "      <td>48.45</td>\n",
       "      <td>403.78</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>21467.99</td>\n",
       "      <td>-6170</td>\n",
       "      <td>4948.56</td>\n",
       "      <td>20246.55</td>\n",
       "      <td>20227.25</td>\n",
       "      <td>-19.30</td>\n",
       "      <td>265.81</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>20227.25</td>\n",
       "      <td>-5945</td>\n",
       "      <td>7956.88</td>\n",
       "      <td>22239.13</td>\n",
       "      <td>22267.31</td>\n",
       "      <td>28.18</td>\n",
       "      <td>197.02</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>22267.31</td>\n",
       "      <td>-5780</td>\n",
       "      <td>5977.77</td>\n",
       "      <td>22465.08</td>\n",
       "      <td>22511.93</td>\n",
       "      <td>46.85</td>\n",
       "      <td>186.88</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>22511.93</td>\n",
       "      <td>-5895</td>\n",
       "      <td>5989.00</td>\n",
       "      <td>22605.93</td>\n",
       "      <td>22574.78</td>\n",
       "      <td>-31.15</td>\n",
       "      <td>210.26</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>22574.78</td>\n",
       "      <td>-5760</td>\n",
       "      <td>6292.65</td>\n",
       "      <td>23107.43</td>\n",
       "      <td>23200.34</td>\n",
       "      <td>92.91</td>\n",
       "      <td>209.19</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>23200.34</td>\n",
       "      <td>-5935</td>\n",
       "      <td>3753.77</td>\n",
       "      <td>21019.11</td>\n",
       "      <td>20963.77</td>\n",
       "      <td>-55.34</td>\n",
       "      <td>114.05</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20963.77</td>\n",
       "      <td>-5490</td>\n",
       "      <td>6918.55</td>\n",
       "      <td>22392.32</td>\n",
       "      <td>22352.14</td>\n",
       "      <td>-40.18</td>\n",
       "      <td>108.21</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22352.14</td>\n",
       "      <td>-5670</td>\n",
       "      <td>6563.68</td>\n",
       "      <td>23245.82</td>\n",
       "      <td>23206.49</td>\n",
       "      <td>-39.33</td>\n",
       "      <td>14.38</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23206.49</td>\n",
       "      <td>-5805</td>\n",
       "      <td>5117.12</td>\n",
       "      <td>22518.61</td>\n",
       "      <td>22516.41</td>\n",
       "      <td>-2.20</td>\n",
       "      <td>28.89</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>22516.41</td>\n",
       "      <td>-6315</td>\n",
       "      <td>8234.24</td>\n",
       "      <td>24435.65</td>\n",
       "      <td>24424.35</td>\n",
       "      <td>-11.30</td>\n",
       "      <td>-30.86</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24424.35</td>\n",
       "      <td>-5860</td>\n",
       "      <td>3230.48</td>\n",
       "      <td>21794.83</td>\n",
       "      <td>21783.77</td>\n",
       "      <td>-11.06</td>\n",
       "      <td>-22.62</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>21783.77</td>\n",
       "      <td>-6315</td>\n",
       "      <td>5967.93</td>\n",
       "      <td>21436.70</td>\n",
       "      <td>21343.34</td>\n",
       "      <td>-93.36</td>\n",
       "      <td>-144.16</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>21343.34</td>\n",
       "      <td>-6070</td>\n",
       "      <td>5946.98</td>\n",
       "      <td>21220.32</td>\n",
       "      <td>21269.16</td>\n",
       "      <td>48.84</td>\n",
       "      <td>-142.17</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>21269.16</td>\n",
       "      <td>-5900</td>\n",
       "      <td>6411.31</td>\n",
       "      <td>21780.47</td>\n",
       "      <td>21628.04</td>\n",
       "      <td>-152.43</td>\n",
       "      <td>-263.45</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Volumen dep. almacenam. ini. (L)  Venta (L)  Llenado dep. almacenam. (L)  \\\n",
       "0                           20300.59      -5785                      7017.89   \n",
       "1                           21522.72      -6040                      7983.27   \n",
       "2                           23302.24      -6005                      4558.82   \n",
       "3                           21823.00      -5820                      5284.44   \n",
       "4                           21257.00      -5975                      6238.65   \n",
       "5                           21639.32      -5895                      7578.84   \n",
       "6                           23420.13      -5820                      4391.04   \n",
       "7                           22048.16      -6135                      7710.20   \n",
       "8                           23568.83      -5700                      3708.58   \n",
       "9                           21671.39      -6240                      6945.83   \n",
       "10                          22417.02      -6635                      8916.91   \n",
       "11                          24664.59      -5835                      3471.00   \n",
       "12                          22355.09      -5905                      5214.81   \n",
       "13                          21648.19      -5865                      5636.35   \n",
       "14                          21467.99      -6170                      4948.56   \n",
       "15                          20227.25      -5945                      7956.88   \n",
       "16                          22267.31      -5780                      5977.77   \n",
       "17                          22511.93      -5895                      5989.00   \n",
       "18                          22574.78      -5760                      6292.65   \n",
       "19                          23200.34      -5935                      3753.77   \n",
       "20                          20963.77      -5490                      6918.55   \n",
       "21                          22352.14      -5670                      6563.68   \n",
       "22                          23206.49      -5805                      5117.12   \n",
       "23                          22516.41      -6315                      8234.24   \n",
       "24                          24424.35      -5860                      3230.48   \n",
       "25                          21783.77      -6315                      5967.93   \n",
       "26                          21343.34      -6070                      5946.98   \n",
       "27                          21269.16      -5900                      6411.31   \n",
       "\n",
       "    Volumen dep. almacenam. fin. teor. (L)  Volumen dep. almacenam. fin. (L)  \\\n",
       "0                                 21533.48                          21522.72   \n",
       "1                                 23465.99                          23302.24   \n",
       "2                                 21856.06                          21823.00   \n",
       "3                                 21287.44                          21257.00   \n",
       "4                                 21520.65                          21639.32   \n",
       "5                                 23323.16                          23420.13   \n",
       "6                                 21991.17                          22048.16   \n",
       "7                                 23623.36                          23568.83   \n",
       "8                                 21577.41                          21671.39   \n",
       "9                                 22377.22                          22417.02   \n",
       "10                                24698.93                          24664.59   \n",
       "11                                22300.59                          22355.09   \n",
       "12                                21664.90                          21648.19   \n",
       "13                                21419.54                          21467.99   \n",
       "14                                20246.55                          20227.25   \n",
       "15                                22239.13                          22267.31   \n",
       "16                                22465.08                          22511.93   \n",
       "17                                22605.93                          22574.78   \n",
       "18                                23107.43                          23200.34   \n",
       "19                                21019.11                          20963.77   \n",
       "20                                22392.32                          22352.14   \n",
       "21                                23245.82                          23206.49   \n",
       "22                                22518.61                          22516.41   \n",
       "23                                24435.65                          24424.35   \n",
       "24                                21794.83                          21783.77   \n",
       "25                                21436.70                          21343.34   \n",
       "26                                21220.32                          21269.16   \n",
       "27                                21780.47                          21628.04   \n",
       "\n",
       "    Variacion  Variacion Acum.  Fugando combustible  \n",
       "0      -10.76           234.60                    0  \n",
       "1     -163.75            35.97                    0  \n",
       "2      -33.06           -38.51                    0  \n",
       "3      -30.44           -45.57                    0  \n",
       "4      118.67            -4.62                    0  \n",
       "5       96.97           105.45                    0  \n",
       "6       56.99           158.48                    0  \n",
       "7      -54.53            48.56                    0  \n",
       "8       93.98           141.14                    0  \n",
       "9       39.80           113.87                    0  \n",
       "10     -34.34            90.29                    0  \n",
       "11      54.50           308.54                    0  \n",
       "12     -16.71           324.89                    0  \n",
       "13      48.45           403.78                    0  \n",
       "14     -19.30           265.81                    1  \n",
       "15      28.18           197.02                    1  \n",
       "16      46.85           186.88                    1  \n",
       "17     -31.15           210.26                    1  \n",
       "18      92.91           209.19                    1  \n",
       "19     -55.34           114.05                    1  \n",
       "20     -40.18           108.21                    1  \n",
       "21     -39.33            14.38                    1  \n",
       "22      -2.20            28.89                    1  \n",
       "23     -11.30           -30.86                    1  \n",
       "24     -11.06           -22.62                    1  \n",
       "25     -93.36          -144.16                    1  \n",
       "26      48.84          -142.17                    1  \n",
       "27    -152.43          -263.45                    1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bad_formatted_fields = [\n",
    "    'Volumen dep. almacenam. ini. (L)',\n",
    "    'Llenado dep. almacenam. (L)',\n",
    "    'Volumen dep. almacenam. fin. teor. (L)',\n",
    "    'Volumen dep. almacenam. fin. (L)',\n",
    "    'Variacion',\n",
    "    'Variacion Acum.',\n",
    "]\n",
    "\n",
    "for column in bad_formatted_fields:\n",
    "    df[column] = df[column].str.replace(',', '.')\n",
    "    df[column] = pd.to_numeric(df[column])\n",
    "\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descripción de campos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- TODO -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesados de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Volumen dep. almacenam. ini. (L)</th>\n",
       "      <th>Venta (L)</th>\n",
       "      <th>Llenado dep. almacenam. (L)</th>\n",
       "      <th>Volumen dep. almacenam. fin. teor. (L)</th>\n",
       "      <th>Volumen dep. almacenam. fin. (L)</th>\n",
       "      <th>Variacion</th>\n",
       "      <th>Variacion Acum.</th>\n",
       "      <th>Fugando combustible</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.016528</td>\n",
       "      <td>-5785</td>\n",
       "      <td>0.666044</td>\n",
       "      <td>0.289043</td>\n",
       "      <td>0.291947</td>\n",
       "      <td>0.541711</td>\n",
       "      <td>0.746444</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.291947</td>\n",
       "      <td>-6040</td>\n",
       "      <td>0.835813</td>\n",
       "      <td>0.723083</td>\n",
       "      <td>0.692980</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.448751</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.692980</td>\n",
       "      <td>-6005</td>\n",
       "      <td>0.233598</td>\n",
       "      <td>0.361494</td>\n",
       "      <td>0.359619</td>\n",
       "      <td>0.462751</td>\n",
       "      <td>0.337125</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.359619</td>\n",
       "      <td>-5820</td>\n",
       "      <td>0.361204</td>\n",
       "      <td>0.233783</td>\n",
       "      <td>0.232065</td>\n",
       "      <td>0.472027</td>\n",
       "      <td>0.326544</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.232065</td>\n",
       "      <td>-5975</td>\n",
       "      <td>0.529009</td>\n",
       "      <td>0.286162</td>\n",
       "      <td>0.318224</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.387917</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.318224</td>\n",
       "      <td>-5895</td>\n",
       "      <td>0.764691</td>\n",
       "      <td>0.691003</td>\n",
       "      <td>0.719548</td>\n",
       "      <td>0.923164</td>\n",
       "      <td>0.552883</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.719548</td>\n",
       "      <td>-5820</td>\n",
       "      <td>0.204093</td>\n",
       "      <td>0.391840</td>\n",
       "      <td>0.410361</td>\n",
       "      <td>0.781602</td>\n",
       "      <td>0.632361</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.410361</td>\n",
       "      <td>-6135</td>\n",
       "      <td>0.787791</td>\n",
       "      <td>0.758428</td>\n",
       "      <td>0.753059</td>\n",
       "      <td>0.386729</td>\n",
       "      <td>0.467620</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.753059</td>\n",
       "      <td>-5700</td>\n",
       "      <td>0.084077</td>\n",
       "      <td>0.298910</td>\n",
       "      <td>0.325452</td>\n",
       "      <td>0.912577</td>\n",
       "      <td>0.606373</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.325452</td>\n",
       "      <td>-6240</td>\n",
       "      <td>0.653371</td>\n",
       "      <td>0.478546</td>\n",
       "      <td>0.493487</td>\n",
       "      <td>0.720735</td>\n",
       "      <td>0.565502</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.493487</td>\n",
       "      <td>-6635</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.458218</td>\n",
       "      <td>0.530162</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-5835</td>\n",
       "      <td>0.042297</td>\n",
       "      <td>0.461335</td>\n",
       "      <td>0.479531</td>\n",
       "      <td>0.772785</td>\n",
       "      <td>0.857261</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.479531</td>\n",
       "      <td>-5905</td>\n",
       "      <td>0.348959</td>\n",
       "      <td>0.318560</td>\n",
       "      <td>0.320223</td>\n",
       "      <td>0.520643</td>\n",
       "      <td>0.881765</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.320223</td>\n",
       "      <td>-5865</td>\n",
       "      <td>0.423090</td>\n",
       "      <td>0.263452</td>\n",
       "      <td>0.279613</td>\n",
       "      <td>0.751363</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.279613</td>\n",
       "      <td>-6170</td>\n",
       "      <td>0.302137</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.511472</td>\n",
       "      <td>0.793220</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5945</td>\n",
       "      <td>0.831172</td>\n",
       "      <td>0.447531</td>\n",
       "      <td>0.459748</td>\n",
       "      <td>0.679591</td>\n",
       "      <td>0.690122</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.459748</td>\n",
       "      <td>-5780</td>\n",
       "      <td>0.483131</td>\n",
       "      <td>0.498280</td>\n",
       "      <td>0.514876</td>\n",
       "      <td>0.745698</td>\n",
       "      <td>0.674925</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.514876</td>\n",
       "      <td>-5895</td>\n",
       "      <td>0.485106</td>\n",
       "      <td>0.529914</td>\n",
       "      <td>0.529040</td>\n",
       "      <td>0.469513</td>\n",
       "      <td>0.709965</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.529040</td>\n",
       "      <td>-5760</td>\n",
       "      <td>0.538505</td>\n",
       "      <td>0.642551</td>\n",
       "      <td>0.670016</td>\n",
       "      <td>0.908788</td>\n",
       "      <td>0.708361</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.670016</td>\n",
       "      <td>-5935</td>\n",
       "      <td>0.092024</td>\n",
       "      <td>0.173516</td>\n",
       "      <td>0.165982</td>\n",
       "      <td>0.383861</td>\n",
       "      <td>0.565772</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.165982</td>\n",
       "      <td>-5490</td>\n",
       "      <td>0.648574</td>\n",
       "      <td>0.481938</td>\n",
       "      <td>0.478866</td>\n",
       "      <td>0.437540</td>\n",
       "      <td>0.557019</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.478866</td>\n",
       "      <td>-5670</td>\n",
       "      <td>0.586167</td>\n",
       "      <td>0.673633</td>\n",
       "      <td>0.671402</td>\n",
       "      <td>0.440550</td>\n",
       "      <td>0.416393</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.671402</td>\n",
       "      <td>-5805</td>\n",
       "      <td>0.331779</td>\n",
       "      <td>0.510302</td>\n",
       "      <td>0.515886</td>\n",
       "      <td>0.572020</td>\n",
       "      <td>0.438140</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.515886</td>\n",
       "      <td>-6315</td>\n",
       "      <td>0.879948</td>\n",
       "      <td>0.940868</td>\n",
       "      <td>0.945859</td>\n",
       "      <td>0.539799</td>\n",
       "      <td>0.348590</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.945859</td>\n",
       "      <td>-5860</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.347742</td>\n",
       "      <td>0.350778</td>\n",
       "      <td>0.540649</td>\n",
       "      <td>0.360940</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.350778</td>\n",
       "      <td>-6315</td>\n",
       "      <td>0.481400</td>\n",
       "      <td>0.267306</td>\n",
       "      <td>0.251522</td>\n",
       "      <td>0.249239</td>\n",
       "      <td>0.178784</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.251522</td>\n",
       "      <td>-6070</td>\n",
       "      <td>0.477716</td>\n",
       "      <td>0.218708</td>\n",
       "      <td>0.234805</td>\n",
       "      <td>0.752744</td>\n",
       "      <td>0.181766</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.234805</td>\n",
       "      <td>-5900</td>\n",
       "      <td>0.559372</td>\n",
       "      <td>0.344517</td>\n",
       "      <td>0.315682</td>\n",
       "      <td>0.040082</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Volumen dep. almacenam. ini. (L)  Venta (L)  Llenado dep. almacenam. (L)  \\\n",
       "0                           0.016528      -5785                     0.666044   \n",
       "1                           0.291947      -6040                     0.835813   \n",
       "2                           0.692980      -6005                     0.233598   \n",
       "3                           0.359619      -5820                     0.361204   \n",
       "4                           0.232065      -5975                     0.529009   \n",
       "5                           0.318224      -5895                     0.764691   \n",
       "6                           0.719548      -5820                     0.204093   \n",
       "7                           0.410361      -6135                     0.787791   \n",
       "8                           0.753059      -5700                     0.084077   \n",
       "9                           0.325452      -6240                     0.653371   \n",
       "10                          0.493487      -6635                     1.000000   \n",
       "11                          1.000000      -5835                     0.042297   \n",
       "12                          0.479531      -5905                     0.348959   \n",
       "13                          0.320223      -5865                     0.423090   \n",
       "14                          0.279613      -6170                     0.302137   \n",
       "15                          0.000000      -5945                     0.831172   \n",
       "16                          0.459748      -5780                     0.483131   \n",
       "17                          0.514876      -5895                     0.485106   \n",
       "18                          0.529040      -5760                     0.538505   \n",
       "19                          0.670016      -5935                     0.092024   \n",
       "20                          0.165982      -5490                     0.648574   \n",
       "21                          0.478866      -5670                     0.586167   \n",
       "22                          0.671402      -5805                     0.331779   \n",
       "23                          0.515886      -6315                     0.879948   \n",
       "24                          0.945859      -5860                     0.000000   \n",
       "25                          0.350778      -6315                     0.481400   \n",
       "26                          0.251522      -6070                     0.477716   \n",
       "27                          0.234805      -5900                     0.559372   \n",
       "\n",
       "    Volumen dep. almacenam. fin. teor. (L)  Volumen dep. almacenam. fin. (L)  \\\n",
       "0                                 0.289043                          0.291947   \n",
       "1                                 0.723083                          0.692980   \n",
       "2                                 0.361494                          0.359619   \n",
       "3                                 0.233783                          0.232065   \n",
       "4                                 0.286162                          0.318224   \n",
       "5                                 0.691003                          0.719548   \n",
       "6                                 0.391840                          0.410361   \n",
       "7                                 0.758428                          0.753059   \n",
       "8                                 0.298910                          0.325452   \n",
       "9                                 0.478546                          0.493487   \n",
       "10                                1.000000                          1.000000   \n",
       "11                                0.461335                          0.479531   \n",
       "12                                0.318560                          0.320223   \n",
       "13                                0.263452                          0.279613   \n",
       "14                                0.000000                          0.000000   \n",
       "15                                0.447531                          0.459748   \n",
       "16                                0.498280                          0.514876   \n",
       "17                                0.529914                          0.529040   \n",
       "18                                0.642551                          0.670016   \n",
       "19                                0.173516                          0.165982   \n",
       "20                                0.481938                          0.478866   \n",
       "21                                0.673633                          0.671402   \n",
       "22                                0.510302                          0.515886   \n",
       "23                                0.940868                          0.945859   \n",
       "24                                0.347742                          0.350778   \n",
       "25                                0.267306                          0.251522   \n",
       "26                                0.218708                          0.234805   \n",
       "27                                0.344517                          0.315682   \n",
       "\n",
       "    Variacion  Variacion Acum.  Fugando combustible  \n",
       "0    0.541711         0.746444                    0  \n",
       "1    0.000000         0.448751                    0  \n",
       "2    0.462751         0.337125                    0  \n",
       "3    0.472027         0.326544                    0  \n",
       "4    1.000000         0.387917                    0  \n",
       "5    0.923164         0.552883                    0  \n",
       "6    0.781602         0.632361                    0  \n",
       "7    0.386729         0.467620                    0  \n",
       "8    0.912577         0.606373                    0  \n",
       "9    0.720735         0.565502                    0  \n",
       "10   0.458218         0.530162                    0  \n",
       "11   0.772785         0.857261                    0  \n",
       "12   0.520643         0.881765                    0  \n",
       "13   0.751363         1.000000                    0  \n",
       "14   0.511472         0.793220                    1  \n",
       "15   0.679591         0.690122                    1  \n",
       "16   0.745698         0.674925                    1  \n",
       "17   0.469513         0.709965                    1  \n",
       "18   0.908788         0.708361                    1  \n",
       "19   0.383861         0.565772                    1  \n",
       "20   0.437540         0.557019                    1  \n",
       "21   0.440550         0.416393                    1  \n",
       "22   0.572020         0.438140                    1  \n",
       "23   0.539799         0.348590                    1  \n",
       "24   0.540649         0.360940                    1  \n",
       "25   0.249239         0.178784                    1  \n",
       "26   0.752744         0.181766                    1  \n",
       "27   0.040082         0.000000                    1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "numeric_cols = [\n",
    "    'Volumen dep. almacenam. ini. (L)',\n",
    "    'Llenado dep. almacenam. (L)',\n",
    "    'Volumen dep. almacenam. fin. teor. (L)',\n",
    "    'Volumen dep. almacenam. fin. (L)',\n",
    "    'Variacion',\n",
    "    'Variacion Acum.',\n",
    "]\n",
    "\n",
    "for col in numeric_cols:\n",
    "    df[col] = MinMaxScaler().fit_transform(df[[col]])\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementación de modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,020</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m)             │         \u001b[38;5;34m1,020\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m16\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,036</span> (4.05 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,036\u001b[0m (4.05 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,036</span> (4.05 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,036\u001b[0m (4.05 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "OBJECTIVE_VARIABLE = 'Fugando combustible'\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df.drop(columns=OBJECTIVE_VARIABLE),\n",
    "    df[OBJECTIVE_VARIABLE],\n",
    "    stratify=df[OBJECTIVE_VARIABLE]\n",
    ")\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(X_train.shape[1], 1)),\n",
    "    tf.keras.layers.LSTM(15),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - accuracy: 0.4762 - loss: 0.6910 - val_accuracy: 0.4286 - val_loss: 0.7014\n",
      "Epoch 2/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.5238 - loss: 0.6874 - val_accuracy: 0.4286 - val_loss: 0.7076\n",
      "Epoch 3/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.5238 - loss: 0.6855 - val_accuracy: 0.4286 - val_loss: 0.7081\n",
      "Epoch 4/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.5238 - loss: 0.6827 - val_accuracy: 0.4286 - val_loss: 0.7067\n",
      "Epoch 5/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.5714 - loss: 0.6796 - val_accuracy: 0.5714 - val_loss: 0.7039\n",
      "Epoch 6/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.6190 - loss: 0.6765 - val_accuracy: 0.5714 - val_loss: 0.7009\n",
      "Epoch 7/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.6667 - loss: 0.6738 - val_accuracy: 0.5714 - val_loss: 0.6997\n",
      "Epoch 8/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.6190 - loss: 0.6712 - val_accuracy: 0.5714 - val_loss: 0.7032\n",
      "Epoch 9/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.6667 - loss: 0.6684 - val_accuracy: 0.5714 - val_loss: 0.7083\n",
      "Epoch 10/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.6667 - loss: 0.6650 - val_accuracy: 0.5714 - val_loss: 0.7147\n",
      "Epoch 11/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.6667 - loss: 0.6617 - val_accuracy: 0.5714 - val_loss: 0.7197\n",
      "Epoch 12/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.6667 - loss: 0.6584 - val_accuracy: 0.5714 - val_loss: 0.7212\n",
      "Epoch 13/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.6667 - loss: 0.6547 - val_accuracy: 0.5714 - val_loss: 0.7244\n",
      "Epoch 14/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.6667 - loss: 0.6514 - val_accuracy: 0.5714 - val_loss: 0.7214\n",
      "Epoch 15/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.5238 - loss: 0.6497 - val_accuracy: 0.5714 - val_loss: 0.7410\n",
      "Epoch 16/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.6667 - loss: 0.6462 - val_accuracy: 0.5714 - val_loss: 0.7647\n",
      "Epoch 17/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.6667 - loss: 0.6461 - val_accuracy: 0.5714 - val_loss: 0.7645\n",
      "Epoch 18/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.6667 - loss: 0.6436 - val_accuracy: 0.5714 - val_loss: 0.7531\n",
      "Epoch 19/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.6190 - loss: 0.6434 - val_accuracy: 0.5714 - val_loss: 0.7623\n",
      "Epoch 20/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.6667 - loss: 0.6415 - val_accuracy: 0.4286 - val_loss: 0.8589\n",
      "Epoch 21/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.6190 - loss: 0.6760 - val_accuracy: 0.4286 - val_loss: 0.8257\n",
      "Epoch 22/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.6667 - loss: 0.6569 - val_accuracy: 0.5714 - val_loss: 0.7754\n",
      "Epoch 23/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.6667 - loss: 0.6385 - val_accuracy: 0.5714 - val_loss: 0.7634\n",
      "Epoch 24/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.6667 - loss: 0.6368 - val_accuracy: 0.5714 - val_loss: 0.7248\n",
      "Epoch 25/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.5714 - loss: 0.6427 - val_accuracy: 0.2857 - val_loss: 0.7128\n",
      "Epoch 26/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.5238 - loss: 0.6536 - val_accuracy: 0.2857 - val_loss: 0.7151\n",
      "Epoch 27/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.5714 - loss: 0.6466 - val_accuracy: 0.5714 - val_loss: 0.7265\n",
      "Epoch 28/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.5714 - loss: 0.6355 - val_accuracy: 0.5714 - val_loss: 0.7440\n",
      "Epoch 29/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.6667 - loss: 0.6316 - val_accuracy: 0.5714 - val_loss: 0.7606\n",
      "Epoch 30/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.6667 - loss: 0.6339 - val_accuracy: 0.5714 - val_loss: 0.7698\n",
      "Epoch 31/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.6667 - loss: 0.6360 - val_accuracy: 0.5714 - val_loss: 0.7717\n",
      "Epoch 32/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.6667 - loss: 0.6367 - val_accuracy: 0.5714 - val_loss: 0.7649\n",
      "Epoch 33/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.6667 - loss: 0.6340 - val_accuracy: 0.5714 - val_loss: 0.7523\n",
      "Epoch 34/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.6667 - loss: 0.6295 - val_accuracy: 0.5714 - val_loss: 0.7377\n",
      "Epoch 35/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.6667 - loss: 0.6258 - val_accuracy: 0.4286 - val_loss: 0.7251\n",
      "Epoch 36/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.6667 - loss: 0.6248 - val_accuracy: 0.4286 - val_loss: 0.7176\n",
      "Epoch 37/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.7143 - loss: 0.6259 - val_accuracy: 0.4286 - val_loss: 0.7158\n",
      "Epoch 38/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.7143 - loss: 0.6256 - val_accuracy: 0.4286 - val_loss: 0.7202\n",
      "Epoch 39/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.7619 - loss: 0.6215 - val_accuracy: 0.4286 - val_loss: 0.7270\n",
      "Epoch 40/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.6667 - loss: 0.6180 - val_accuracy: 0.4286 - val_loss: 0.7361\n",
      "Epoch 41/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.6667 - loss: 0.6159 - val_accuracy: 0.4286 - val_loss: 0.7445\n",
      "Epoch 42/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.6667 - loss: 0.6150 - val_accuracy: 0.4286 - val_loss: 0.7494\n",
      "Epoch 43/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.6667 - loss: 0.6134 - val_accuracy: 0.4286 - val_loss: 0.7511\n",
      "Epoch 44/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.6667 - loss: 0.6111 - val_accuracy: 0.4286 - val_loss: 0.7461\n",
      "Epoch 45/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.6667 - loss: 0.6063 - val_accuracy: 0.4286 - val_loss: 0.7403\n",
      "Epoch 46/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.7619 - loss: 0.6025 - val_accuracy: 0.5714 - val_loss: 0.7390\n",
      "Epoch 47/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.7619 - loss: 0.6005 - val_accuracy: 0.4286 - val_loss: 0.7440\n",
      "Epoch 48/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.6667 - loss: 0.6005 - val_accuracy: 0.5714 - val_loss: 0.7463\n",
      "Epoch 49/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.7619 - loss: 0.5963 - val_accuracy: 0.5714 - val_loss: 0.7511\n",
      "Epoch 50/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.7619 - loss: 0.5915 - val_accuracy: 0.4286 - val_loss: 0.7603\n",
      "Epoch 51/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.7619 - loss: 0.5858 - val_accuracy: 0.4286 - val_loss: 0.7746\n",
      "Epoch 52/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.7143 - loss: 0.5794 - val_accuracy: 0.4286 - val_loss: 0.7941\n",
      "Epoch 53/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.7143 - loss: 0.5726 - val_accuracy: 0.5714 - val_loss: 0.8115\n",
      "Epoch 54/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.7143 - loss: 0.5647 - val_accuracy: 0.5714 - val_loss: 0.8358\n",
      "Epoch 55/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.8095 - loss: 0.5572 - val_accuracy: 0.5714 - val_loss: 0.8669\n",
      "Epoch 56/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.7619 - loss: 0.5483 - val_accuracy: 0.4286 - val_loss: 1.0128\n",
      "Epoch 57/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.6190 - loss: 0.6326 - val_accuracy: 0.5714 - val_loss: 1.0012\n",
      "Epoch 58/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.7619 - loss: 0.6126 - val_accuracy: 0.5714 - val_loss: 0.9793\n",
      "Epoch 59/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.7619 - loss: 0.5547 - val_accuracy: 0.2857 - val_loss: 0.8450\n",
      "Epoch 60/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.6190 - loss: 0.5781 - val_accuracy: 0.2857 - val_loss: 0.8344\n",
      "Epoch 61/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.6667 - loss: 0.5905 - val_accuracy: 0.4286 - val_loss: 0.8754\n",
      "Epoch 62/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.7619 - loss: 0.5260 - val_accuracy: 0.4286 - val_loss: 0.8690\n",
      "Epoch 63/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.7619 - loss: 0.5354 - val_accuracy: 0.4286 - val_loss: 0.8780\n",
      "Epoch 64/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.7619 - loss: 0.5235 - val_accuracy: 0.2857 - val_loss: 0.9064\n",
      "Epoch 65/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.7143 - loss: 0.5072 - val_accuracy: 0.2857 - val_loss: 0.9364\n",
      "Epoch 66/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.7143 - loss: 0.4880 - val_accuracy: 0.2857 - val_loss: 0.9526\n",
      "Epoch 67/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.7619 - loss: 0.4738 - val_accuracy: 0.4286 - val_loss: 0.8485\n",
      "Epoch 68/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.7619 - loss: 0.5116 - val_accuracy: 0.5714 - val_loss: 1.0504\n",
      "Epoch 69/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.7619 - loss: 0.5116 - val_accuracy: 0.4286 - val_loss: 1.0821\n",
      "Epoch 70/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.8095 - loss: 0.4848 - val_accuracy: 0.2857 - val_loss: 1.0766\n",
      "Epoch 71/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8095 - loss: 0.4534 - val_accuracy: 0.2857 - val_loss: 1.0041\n",
      "Epoch 72/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.8095 - loss: 0.4305 - val_accuracy: 0.2857 - val_loss: 0.8327\n",
      "Epoch 73/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.7619 - loss: 0.4718 - val_accuracy: 0.2857 - val_loss: 0.8329\n",
      "Epoch 74/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.7619 - loss: 0.4740 - val_accuracy: 0.2857 - val_loss: 0.8819\n",
      "Epoch 75/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.8095 - loss: 0.4547 - val_accuracy: 0.2857 - val_loss: 0.9619\n",
      "Epoch 76/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.8095 - loss: 0.4293 - val_accuracy: 0.2857 - val_loss: 1.0411\n",
      "Epoch 77/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.8095 - loss: 0.3996 - val_accuracy: 0.2857 - val_loss: 1.1219\n",
      "Epoch 78/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.8095 - loss: 0.3902 - val_accuracy: 0.2857 - val_loss: 1.2544\n",
      "Epoch 79/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.7619 - loss: 0.4116 - val_accuracy: 0.2857 - val_loss: 1.2639\n",
      "Epoch 80/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.7619 - loss: 0.4080 - val_accuracy: 0.2857 - val_loss: 1.2239\n",
      "Epoch 81/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.8095 - loss: 0.3792 - val_accuracy: 0.2857 - val_loss: 1.1738\n",
      "Epoch 82/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8571 - loss: 0.3530 - val_accuracy: 0.2857 - val_loss: 1.1183\n",
      "Epoch 83/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.8095 - loss: 0.3537 - val_accuracy: 0.4286 - val_loss: 1.0454\n",
      "Epoch 84/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.8571 - loss: 0.3540 - val_accuracy: 0.4286 - val_loss: 1.0427\n",
      "Epoch 85/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.8571 - loss: 0.3401 - val_accuracy: 0.2857 - val_loss: 2.0147\n",
      "Epoch 86/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.7143 - loss: 1.2885 - val_accuracy: 0.1429 - val_loss: 1.7870\n",
      "Epoch 87/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.6667 - loss: 1.1261 - val_accuracy: 0.2857 - val_loss: 1.6591\n",
      "Epoch 88/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.6667 - loss: 0.6533 - val_accuracy: 0.2857 - val_loss: 1.2834\n",
      "Epoch 89/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.8095 - loss: 0.3500 - val_accuracy: 0.4286 - val_loss: 1.0471\n",
      "Epoch 90/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.8095 - loss: 0.4440 - val_accuracy: 0.5714 - val_loss: 0.7957\n",
      "Epoch 91/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.6190 - loss: 0.6055 - val_accuracy: 0.7143 - val_loss: 0.7295\n",
      "Epoch 92/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.5238 - loss: 0.6777 - val_accuracy: 0.8571 - val_loss: 0.7421\n",
      "Epoch 93/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.6190 - loss: 0.6532 - val_accuracy: 0.5714 - val_loss: 0.7632\n",
      "Epoch 94/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.6190 - loss: 0.5498 - val_accuracy: 0.5714 - val_loss: 0.8110\n",
      "Epoch 95/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.8095 - loss: 0.4237 - val_accuracy: 0.4286 - val_loss: 0.8530\n",
      "Epoch 96/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.7619 - loss: 0.3889 - val_accuracy: 0.2857 - val_loss: 0.8991\n",
      "Epoch 97/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.8095 - loss: 0.4032 - val_accuracy: 0.2857 - val_loss: 0.9920\n",
      "Epoch 98/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.7143 - loss: 0.4201 - val_accuracy: 0.4286 - val_loss: 1.1017\n",
      "Epoch 99/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.6667 - loss: 0.4307 - val_accuracy: 0.4286 - val_loss: 1.1776\n",
      "Epoch 100/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.7619 - loss: 0.4499 - val_accuracy: 0.2857 - val_loss: 1.1837\n",
      "Epoch 101/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.7619 - loss: 0.4458 - val_accuracy: 0.2857 - val_loss: 1.1471\n",
      "Epoch 102/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.8095 - loss: 0.4082 - val_accuracy: 0.2857 - val_loss: 1.1025\n",
      "Epoch 103/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.8571 - loss: 0.3714 - val_accuracy: 0.2857 - val_loss: 1.0714\n",
      "Epoch 104/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.8095 - loss: 0.3631 - val_accuracy: 0.5714 - val_loss: 1.0543\n",
      "Epoch 105/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.8095 - loss: 0.3725 - val_accuracy: 0.4286 - val_loss: 1.0403\n",
      "Epoch 106/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.8095 - loss: 0.3751 - val_accuracy: 0.4286 - val_loss: 1.0240\n",
      "Epoch 107/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.8571 - loss: 0.3678 - val_accuracy: 0.4286 - val_loss: 0.9995\n",
      "Epoch 108/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.8095 - loss: 0.3602 - val_accuracy: 0.4286 - val_loss: 0.9640\n",
      "Epoch 109/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.8095 - loss: 0.3548 - val_accuracy: 0.5714 - val_loss: 0.9337\n",
      "Epoch 110/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.8571 - loss: 0.3471 - val_accuracy: 0.5714 - val_loss: 0.9357\n",
      "Epoch 111/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.9048 - loss: 0.3314 - val_accuracy: 0.5714 - val_loss: 0.9302\n",
      "Epoch 112/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.8571 - loss: 0.3182 - val_accuracy: 0.5714 - val_loss: 0.9247\n",
      "Epoch 113/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.8571 - loss: 0.3163 - val_accuracy: 0.4286 - val_loss: 0.9328\n",
      "Epoch 114/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.8095 - loss: 0.3227 - val_accuracy: 0.4286 - val_loss: 0.9695\n",
      "Epoch 115/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.8095 - loss: 0.3161 - val_accuracy: 0.4286 - val_loss: 1.0263\n",
      "Epoch 116/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.8571 - loss: 0.3036 - val_accuracy: 0.4286 - val_loss: 1.0811\n",
      "Epoch 117/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.9048 - loss: 0.3009 - val_accuracy: 0.4286 - val_loss: 1.1275\n",
      "Epoch 118/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9048 - loss: 0.2981 - val_accuracy: 0.2857 - val_loss: 1.1744\n",
      "Epoch 119/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.9048 - loss: 0.2890 - val_accuracy: 0.4286 - val_loss: 1.1903\n",
      "Epoch 120/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9524 - loss: 0.2833 - val_accuracy: 0.4286 - val_loss: 1.2165\n",
      "Epoch 121/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9048 - loss: 0.2868 - val_accuracy: 0.4286 - val_loss: 1.2210\n",
      "Epoch 122/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.9048 - loss: 0.2863 - val_accuracy: 0.4286 - val_loss: 1.2016\n",
      "Epoch 123/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.9048 - loss: 0.2779 - val_accuracy: 0.5714 - val_loss: 1.1700\n",
      "Epoch 124/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.9048 - loss: 0.2728 - val_accuracy: 0.5714 - val_loss: 1.1485\n",
      "Epoch 125/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.9048 - loss: 0.2724 - val_accuracy: 0.5714 - val_loss: 1.1416\n",
      "Epoch 126/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9048 - loss: 0.2677 - val_accuracy: 0.5714 - val_loss: 1.1423\n",
      "Epoch 127/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.9048 - loss: 0.2615 - val_accuracy: 0.5714 - val_loss: 1.1521\n",
      "Epoch 128/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.8571 - loss: 0.2619 - val_accuracy: 0.5714 - val_loss: 1.1751\n",
      "Epoch 129/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.8571 - loss: 0.2599 - val_accuracy: 0.5714 - val_loss: 1.2052\n",
      "Epoch 130/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9048 - loss: 0.2531 - val_accuracy: 0.5714 - val_loss: 1.2316\n",
      "Epoch 131/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9048 - loss: 0.2514 - val_accuracy: 0.5714 - val_loss: 1.2523\n",
      "Epoch 132/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9048 - loss: 0.2493 - val_accuracy: 0.5714 - val_loss: 1.2691\n",
      "Epoch 133/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9524 - loss: 0.2437 - val_accuracy: 0.5714 - val_loss: 1.2744\n",
      "Epoch 134/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9524 - loss: 0.2416 - val_accuracy: 0.5714 - val_loss: 1.2592\n",
      "Epoch 135/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.9524 - loss: 0.2398 - val_accuracy: 0.5714 - val_loss: 1.2320\n",
      "Epoch 136/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9524 - loss: 0.2350 - val_accuracy: 0.5714 - val_loss: 1.2133\n",
      "Epoch 137/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9524 - loss: 0.2330 - val_accuracy: 0.5714 - val_loss: 1.2105\n",
      "Epoch 138/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.9048 - loss: 0.2313 - val_accuracy: 0.5714 - val_loss: 1.2182\n",
      "Epoch 139/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.9524 - loss: 0.2268 - val_accuracy: 0.5714 - val_loss: 1.2351\n",
      "Epoch 140/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9524 - loss: 0.2248 - val_accuracy: 0.5714 - val_loss: 1.2566\n",
      "Epoch 141/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9524 - loss: 0.2230 - val_accuracy: 0.5714 - val_loss: 1.2730\n",
      "Epoch 142/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.9524 - loss: 0.2198 - val_accuracy: 0.5714 - val_loss: 1.2813\n",
      "Epoch 143/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9524 - loss: 0.2186 - val_accuracy: 0.5714 - val_loss: 1.2843\n",
      "Epoch 144/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9524 - loss: 0.2164 - val_accuracy: 0.5714 - val_loss: 1.2829\n",
      "Epoch 145/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9524 - loss: 0.2134 - val_accuracy: 0.5714 - val_loss: 1.2761\n",
      "Epoch 146/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9524 - loss: 0.2121 - val_accuracy: 0.5714 - val_loss: 1.2677\n",
      "Epoch 147/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.9524 - loss: 0.2102 - val_accuracy: 0.5714 - val_loss: 1.2669\n",
      "Epoch 148/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.9524 - loss: 0.2081 - val_accuracy: 0.7143 - val_loss: 1.2757\n",
      "Epoch 149/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9524 - loss: 0.2066 - val_accuracy: 0.7143 - val_loss: 1.2892\n",
      "Epoch 150/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.9524 - loss: 0.2043 - val_accuracy: 0.5714 - val_loss: 1.3058\n",
      "Epoch 151/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.9524 - loss: 0.2022 - val_accuracy: 0.5714 - val_loss: 1.3232\n",
      "Epoch 152/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.9524 - loss: 0.2009 - val_accuracy: 0.5714 - val_loss: 1.3359\n",
      "Epoch 153/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9524 - loss: 0.1989 - val_accuracy: 0.7143 - val_loss: 1.3817\n",
      "Epoch 154/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9524 - loss: 0.2239 - val_accuracy: 0.5714 - val_loss: 1.3220\n",
      "Epoch 155/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.9524 - loss: 0.2015 - val_accuracy: 0.7143 - val_loss: 1.3118\n",
      "Epoch 156/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.9524 - loss: 0.2001 - val_accuracy: 0.7143 - val_loss: 1.3328\n",
      "Epoch 157/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.9524 - loss: 0.1943 - val_accuracy: 0.7143 - val_loss: 1.3641\n",
      "Epoch 158/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9524 - loss: 0.1964 - val_accuracy: 0.7143 - val_loss: 1.3827\n",
      "Epoch 159/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9524 - loss: 0.1909 - val_accuracy: 0.7143 - val_loss: 1.3992\n",
      "Epoch 160/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.9524 - loss: 0.1883 - val_accuracy: 0.5714 - val_loss: 1.4171\n",
      "Epoch 161/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9524 - loss: 0.1914 - val_accuracy: 0.7143 - val_loss: 1.4224\n",
      "Epoch 162/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9524 - loss: 0.1858 - val_accuracy: 0.7143 - val_loss: 1.4287\n",
      "Epoch 163/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.9524 - loss: 0.1843 - val_accuracy: 0.7143 - val_loss: 1.4313\n",
      "Epoch 164/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9524 - loss: 0.1839 - val_accuracy: 0.7143 - val_loss: 1.4215\n",
      "Epoch 165/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9524 - loss: 0.1806 - val_accuracy: 0.7143 - val_loss: 1.4129\n",
      "Epoch 166/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.9524 - loss: 0.1808 - val_accuracy: 0.7143 - val_loss: 1.4248\n",
      "Epoch 167/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9524 - loss: 0.1792 - val_accuracy: 0.7143 - val_loss: 1.4545\n",
      "Epoch 168/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9524 - loss: 0.1756 - val_accuracy: 0.7143 - val_loss: 1.4820\n",
      "Epoch 169/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9524 - loss: 0.1758 - val_accuracy: 0.7143 - val_loss: 1.4935\n",
      "Epoch 170/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.9524 - loss: 0.1749 - val_accuracy: 0.7143 - val_loss: 1.4892\n",
      "Epoch 171/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9524 - loss: 0.1722 - val_accuracy: 0.7143 - val_loss: 1.4793\n",
      "Epoch 172/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9524 - loss: 0.1715 - val_accuracy: 0.7143 - val_loss: 1.4792\n",
      "Epoch 173/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.9524 - loss: 0.1701 - val_accuracy: 0.7143 - val_loss: 1.4936\n",
      "Epoch 174/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9524 - loss: 0.1684 - val_accuracy: 0.7143 - val_loss: 1.5107\n",
      "Epoch 175/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.9524 - loss: 0.1679 - val_accuracy: 0.7143 - val_loss: 1.5187\n",
      "Epoch 176/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9524 - loss: 0.1663 - val_accuracy: 0.7143 - val_loss: 1.5183\n",
      "Epoch 177/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9524 - loss: 0.1645 - val_accuracy: 0.7143 - val_loss: 1.5183\n",
      "Epoch 178/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9524 - loss: 0.1640 - val_accuracy: 0.7143 - val_loss: 1.5262\n",
      "Epoch 179/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9524 - loss: 0.1628 - val_accuracy: 0.7143 - val_loss: 1.5398\n",
      "Epoch 180/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.9524 - loss: 0.1612 - val_accuracy: 0.7143 - val_loss: 1.5506\n",
      "Epoch 181/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.9524 - loss: 0.1603 - val_accuracy: 0.7143 - val_loss: 1.5525\n",
      "Epoch 182/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.9524 - loss: 0.1593 - val_accuracy: 0.7143 - val_loss: 1.5478\n",
      "Epoch 183/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.9524 - loss: 0.1580 - val_accuracy: 0.7143 - val_loss: 1.5457\n",
      "Epoch 184/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.9524 - loss: 0.1571 - val_accuracy: 0.7143 - val_loss: 1.5538\n",
      "Epoch 185/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9524 - loss: 0.1560 - val_accuracy: 0.7143 - val_loss: 1.5700\n",
      "Epoch 186/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9524 - loss: 0.1548 - val_accuracy: 0.7143 - val_loss: 1.5848\n",
      "Epoch 187/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.9524 - loss: 0.1540 - val_accuracy: 0.7143 - val_loss: 1.5911\n",
      "Epoch 188/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.9524 - loss: 0.1530 - val_accuracy: 0.7143 - val_loss: 1.5893\n",
      "Epoch 189/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.9524 - loss: 0.1517 - val_accuracy: 0.7143 - val_loss: 1.5863\n",
      "Epoch 190/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.9524 - loss: 0.1509 - val_accuracy: 0.7143 - val_loss: 1.5897\n",
      "Epoch 191/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9524 - loss: 0.1500 - val_accuracy: 0.7143 - val_loss: 1.6010\n",
      "Epoch 192/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9524 - loss: 0.1489 - val_accuracy: 0.7143 - val_loss: 1.6145\n",
      "Epoch 193/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9524 - loss: 0.1480 - val_accuracy: 0.7143 - val_loss: 1.6240\n",
      "Epoch 194/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.9524 - loss: 0.1471 - val_accuracy: 0.7143 - val_loss: 1.6276\n",
      "Epoch 195/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.9524 - loss: 0.1461 - val_accuracy: 0.7143 - val_loss: 1.6284\n",
      "Epoch 196/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9524 - loss: 0.1453 - val_accuracy: 0.7143 - val_loss: 1.6314\n",
      "Epoch 197/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.9524 - loss: 0.1444 - val_accuracy: 0.7143 - val_loss: 1.6387\n",
      "Epoch 198/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.9524 - loss: 0.1434 - val_accuracy: 0.7143 - val_loss: 1.6482\n",
      "Epoch 199/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9524 - loss: 0.1425 - val_accuracy: 0.7143 - val_loss: 1.6563\n",
      "Epoch 200/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.9524 - loss: 0.1417 - val_accuracy: 0.7143 - val_loss: 1.6613\n",
      "Epoch 201/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.9524 - loss: 0.1407 - val_accuracy: 0.7143 - val_loss: 1.6650\n",
      "Epoch 202/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.9524 - loss: 0.1398 - val_accuracy: 0.7143 - val_loss: 1.6703\n",
      "Epoch 203/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.9524 - loss: 0.1390 - val_accuracy: 0.7143 - val_loss: 1.6780\n",
      "Epoch 204/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9524 - loss: 0.1381 - val_accuracy: 0.7143 - val_loss: 1.6864\n",
      "Epoch 205/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.9524 - loss: 0.1372 - val_accuracy: 0.7143 - val_loss: 1.6929\n",
      "Epoch 206/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9524 - loss: 0.1363 - val_accuracy: 0.7143 - val_loss: 1.6967\n",
      "Epoch 207/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9524 - loss: 0.1354 - val_accuracy: 0.7143 - val_loss: 1.7000\n",
      "Epoch 208/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.9524 - loss: 0.1346 - val_accuracy: 0.7143 - val_loss: 1.7057\n",
      "Epoch 209/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.9524 - loss: 0.1337 - val_accuracy: 0.7143 - val_loss: 1.7143\n",
      "Epoch 210/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.9524 - loss: 0.1328 - val_accuracy: 0.7143 - val_loss: 1.7234\n",
      "Epoch 211/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9524 - loss: 0.1319 - val_accuracy: 0.7143 - val_loss: 1.7302\n",
      "Epoch 212/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9524 - loss: 0.1310 - val_accuracy: 0.7143 - val_loss: 1.7339\n",
      "Epoch 213/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.9524 - loss: 0.1301 - val_accuracy: 0.7143 - val_loss: 1.7366\n",
      "Epoch 214/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.9524 - loss: 0.1292 - val_accuracy: 0.7143 - val_loss: 1.7412\n",
      "Epoch 215/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9524 - loss: 0.1283 - val_accuracy: 0.7143 - val_loss: 1.7483\n",
      "Epoch 216/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9524 - loss: 0.1274 - val_accuracy: 0.7143 - val_loss: 1.7560\n",
      "Epoch 217/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9524 - loss: 0.1265 - val_accuracy: 0.7143 - val_loss: 1.7631\n",
      "Epoch 218/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.9524 - loss: 0.1256 - val_accuracy: 0.7143 - val_loss: 1.7675\n",
      "Epoch 219/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9524 - loss: 0.1247 - val_accuracy: 0.7143 - val_loss: 1.7716\n",
      "Epoch 220/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.9524 - loss: 0.1238 - val_accuracy: 0.7143 - val_loss: 1.7763\n",
      "Epoch 221/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9524 - loss: 0.1229 - val_accuracy: 0.7143 - val_loss: 1.7823\n",
      "Epoch 222/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9524 - loss: 0.1220 - val_accuracy: 0.7143 - val_loss: 1.7887\n",
      "Epoch 223/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.9524 - loss: 0.1211 - val_accuracy: 0.7143 - val_loss: 1.7941\n",
      "Epoch 224/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9524 - loss: 0.1202 - val_accuracy: 0.7143 - val_loss: 1.7984\n",
      "Epoch 225/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.9524 - loss: 0.1193 - val_accuracy: 0.7143 - val_loss: 1.8028\n",
      "Epoch 226/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.9524 - loss: 0.1184 - val_accuracy: 0.7143 - val_loss: 1.8077\n",
      "Epoch 227/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9524 - loss: 0.1174 - val_accuracy: 0.7143 - val_loss: 1.8128\n",
      "Epoch 228/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.9524 - loss: 0.1165 - val_accuracy: 0.7143 - val_loss: 1.8170\n",
      "Epoch 229/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.9524 - loss: 0.1156 - val_accuracy: 0.7143 - val_loss: 1.8202\n",
      "Epoch 230/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.9524 - loss: 0.1147 - val_accuracy: 0.7143 - val_loss: 1.8236\n",
      "Epoch 231/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.9524 - loss: 0.1138 - val_accuracy: 0.7143 - val_loss: 1.8277\n",
      "Epoch 232/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9524 - loss: 0.1129 - val_accuracy: 0.7143 - val_loss: 1.8321\n",
      "Epoch 233/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9524 - loss: 0.1120 - val_accuracy: 0.7143 - val_loss: 1.8359\n",
      "Epoch 234/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.9524 - loss: 0.1111 - val_accuracy: 0.7143 - val_loss: 1.8388\n",
      "Epoch 235/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.9524 - loss: 0.1102 - val_accuracy: 0.7143 - val_loss: 1.8411\n",
      "Epoch 236/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9524 - loss: 0.1094 - val_accuracy: 0.7143 - val_loss: 1.8437\n",
      "Epoch 237/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.9524 - loss: 0.1085 - val_accuracy: 0.7143 - val_loss: 1.8462\n",
      "Epoch 238/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9524 - loss: 0.1076 - val_accuracy: 0.7143 - val_loss: 1.8482\n",
      "Epoch 239/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9524 - loss: 0.1067 - val_accuracy: 0.7143 - val_loss: 1.8495\n",
      "Epoch 240/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.9524 - loss: 0.1058 - val_accuracy: 0.7143 - val_loss: 1.8507\n",
      "Epoch 241/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9524 - loss: 0.1049 - val_accuracy: 0.7143 - val_loss: 1.8522\n",
      "Epoch 242/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.9524 - loss: 0.1040 - val_accuracy: 0.7143 - val_loss: 1.8539\n",
      "Epoch 243/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9524 - loss: 0.1031 - val_accuracy: 0.7143 - val_loss: 1.8550\n",
      "Epoch 244/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.9524 - loss: 0.1022 - val_accuracy: 0.7143 - val_loss: 1.8552\n",
      "Epoch 245/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.9524 - loss: 0.1013 - val_accuracy: 0.7143 - val_loss: 1.8550\n",
      "Epoch 246/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9524 - loss: 0.1004 - val_accuracy: 0.7143 - val_loss: 1.8548\n",
      "Epoch 247/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.9524 - loss: 0.0994 - val_accuracy: 0.7143 - val_loss: 1.8546\n",
      "Epoch 248/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9524 - loss: 0.0985 - val_accuracy: 0.4286 - val_loss: 3.2365\n",
      "Epoch 249/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.6190 - loss: 1.9624 - val_accuracy: 0.7143 - val_loss: 1.8705\n",
      "Epoch 250/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9524 - loss: 0.1396 - val_accuracy: 0.7143 - val_loss: 1.7104\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x25bdededfd0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    x = X_train,\n",
    "    y = y_train,\n",
    "    epochs = 250,\n",
    "    batch_size = 32,\n",
    "    validation_data = (X_test, y_test),\n",
    "    verbose = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x25be3909890>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "ConfusionMatrixDisplay(\n",
    "    confusion_matrix=confusion_matrix(y_test, (model.predict(X_test) > 0.5).astype(int)),\n",
    "    display_labels=['No Fuga', 'Fuga']\n",
    ").plot(\n",
    "    colorbar=False\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
