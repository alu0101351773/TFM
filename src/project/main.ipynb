{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importación de librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lectura de fichero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Volumen dep. almacenam. ini. (L)</th>\n",
       "      <th>Venta (L)</th>\n",
       "      <th>Llenado dep. almacenam. (L)</th>\n",
       "      <th>Volumen dep. almacenam. fin. teor. (L)</th>\n",
       "      <th>Volumen dep. almacenam. fin. (L)</th>\n",
       "      <th>Variacion</th>\n",
       "      <th>Variacion Acum.</th>\n",
       "      <th>Fugando combustible</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20300,59</td>\n",
       "      <td>-5785</td>\n",
       "      <td>7017,89</td>\n",
       "      <td>21533,48</td>\n",
       "      <td>21522,72</td>\n",
       "      <td>-10,76</td>\n",
       "      <td>234,6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21522,72</td>\n",
       "      <td>-6040</td>\n",
       "      <td>7983,27</td>\n",
       "      <td>23465,99</td>\n",
       "      <td>23302,24</td>\n",
       "      <td>-163,75</td>\n",
       "      <td>35,97</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23302,24</td>\n",
       "      <td>-6005</td>\n",
       "      <td>4558,82</td>\n",
       "      <td>21856,06</td>\n",
       "      <td>21823,0</td>\n",
       "      <td>-33,06</td>\n",
       "      <td>-38,51</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21823,0</td>\n",
       "      <td>-5820</td>\n",
       "      <td>5284,44</td>\n",
       "      <td>21287,44</td>\n",
       "      <td>21257,0</td>\n",
       "      <td>-30,44</td>\n",
       "      <td>-45,57</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21257,0</td>\n",
       "      <td>-5975</td>\n",
       "      <td>6238,65</td>\n",
       "      <td>21520,65</td>\n",
       "      <td>21639,32</td>\n",
       "      <td>118,67</td>\n",
       "      <td>-4,62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>21639,32</td>\n",
       "      <td>-5895</td>\n",
       "      <td>7578,84</td>\n",
       "      <td>23323,16</td>\n",
       "      <td>23420,13</td>\n",
       "      <td>96,97</td>\n",
       "      <td>105,45</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>23420,13</td>\n",
       "      <td>-5820</td>\n",
       "      <td>4391,04</td>\n",
       "      <td>21991,17</td>\n",
       "      <td>22048,16</td>\n",
       "      <td>56,99</td>\n",
       "      <td>158,48</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>22048,16</td>\n",
       "      <td>-6135</td>\n",
       "      <td>7710,2</td>\n",
       "      <td>23623,36</td>\n",
       "      <td>23568,83</td>\n",
       "      <td>-54,53</td>\n",
       "      <td>48,56</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>23568,83</td>\n",
       "      <td>-5700</td>\n",
       "      <td>3708,58</td>\n",
       "      <td>21577,41</td>\n",
       "      <td>21671,39</td>\n",
       "      <td>93,98</td>\n",
       "      <td>141,14</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>21671,39</td>\n",
       "      <td>-6240</td>\n",
       "      <td>6945,83</td>\n",
       "      <td>22377,22</td>\n",
       "      <td>22417,02</td>\n",
       "      <td>39,8</td>\n",
       "      <td>113,87</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>22417,02</td>\n",
       "      <td>-6635</td>\n",
       "      <td>8916,91</td>\n",
       "      <td>24698,93</td>\n",
       "      <td>24664,59</td>\n",
       "      <td>-34,34</td>\n",
       "      <td>90,29</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>24664,59</td>\n",
       "      <td>-5835</td>\n",
       "      <td>3471,0</td>\n",
       "      <td>22300,59</td>\n",
       "      <td>22355,09</td>\n",
       "      <td>54,5</td>\n",
       "      <td>308,54</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>22355,09</td>\n",
       "      <td>-5905</td>\n",
       "      <td>5214,81</td>\n",
       "      <td>21664,9</td>\n",
       "      <td>21648,19</td>\n",
       "      <td>-16,71</td>\n",
       "      <td>324,89</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>21648,19</td>\n",
       "      <td>-5865</td>\n",
       "      <td>5636,35</td>\n",
       "      <td>21419,54</td>\n",
       "      <td>21467,99</td>\n",
       "      <td>48,45</td>\n",
       "      <td>403,78</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>21467,99</td>\n",
       "      <td>-6170</td>\n",
       "      <td>4948,56</td>\n",
       "      <td>20246,55</td>\n",
       "      <td>20227,25</td>\n",
       "      <td>-19,3</td>\n",
       "      <td>265,81</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>20227,25</td>\n",
       "      <td>-5945</td>\n",
       "      <td>7956,88</td>\n",
       "      <td>22239,13</td>\n",
       "      <td>22267,31</td>\n",
       "      <td>28,18</td>\n",
       "      <td>197,02</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>22267,31</td>\n",
       "      <td>-5780</td>\n",
       "      <td>5977,77</td>\n",
       "      <td>22465,08</td>\n",
       "      <td>22511,93</td>\n",
       "      <td>46,85</td>\n",
       "      <td>186,88</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>22511,93</td>\n",
       "      <td>-5895</td>\n",
       "      <td>5989,0</td>\n",
       "      <td>22605,93</td>\n",
       "      <td>22574,78</td>\n",
       "      <td>-31,15</td>\n",
       "      <td>210,26</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>22574,78</td>\n",
       "      <td>-5760</td>\n",
       "      <td>6292,65</td>\n",
       "      <td>23107,43</td>\n",
       "      <td>23200,34</td>\n",
       "      <td>92,91</td>\n",
       "      <td>209,19</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>23200,34</td>\n",
       "      <td>-5935</td>\n",
       "      <td>3753,77</td>\n",
       "      <td>21019,11</td>\n",
       "      <td>20963,77</td>\n",
       "      <td>-55,34</td>\n",
       "      <td>114,05</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20963,77</td>\n",
       "      <td>-5490</td>\n",
       "      <td>6918,55</td>\n",
       "      <td>22392,32</td>\n",
       "      <td>22352,14</td>\n",
       "      <td>-40,18</td>\n",
       "      <td>108,21</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22352,14</td>\n",
       "      <td>-5670</td>\n",
       "      <td>6563,68</td>\n",
       "      <td>23245,82</td>\n",
       "      <td>23206,49</td>\n",
       "      <td>-39,33</td>\n",
       "      <td>14,38</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23206,49</td>\n",
       "      <td>-5805</td>\n",
       "      <td>5117,12</td>\n",
       "      <td>22518,61</td>\n",
       "      <td>22516,41</td>\n",
       "      <td>-2,2</td>\n",
       "      <td>28,89</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>22516,41</td>\n",
       "      <td>-6315</td>\n",
       "      <td>8234,24</td>\n",
       "      <td>24435,65</td>\n",
       "      <td>24424,35</td>\n",
       "      <td>-11,3</td>\n",
       "      <td>-30,86</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24424,35</td>\n",
       "      <td>-5860</td>\n",
       "      <td>3230,48</td>\n",
       "      <td>21794,83</td>\n",
       "      <td>21783,77</td>\n",
       "      <td>-11,06</td>\n",
       "      <td>-22,62</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>21783,77</td>\n",
       "      <td>-6315</td>\n",
       "      <td>5967,93</td>\n",
       "      <td>21436,7</td>\n",
       "      <td>21343,34</td>\n",
       "      <td>-93,36</td>\n",
       "      <td>-144,16</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>21343,34</td>\n",
       "      <td>-6070</td>\n",
       "      <td>5946,98</td>\n",
       "      <td>21220,32</td>\n",
       "      <td>21269,16</td>\n",
       "      <td>48,84</td>\n",
       "      <td>-142,17</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>21269,16</td>\n",
       "      <td>-5900</td>\n",
       "      <td>6411,31</td>\n",
       "      <td>21780,47</td>\n",
       "      <td>21628,04</td>\n",
       "      <td>-152,43</td>\n",
       "      <td>-263,45</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Volumen dep. almacenam. ini. (L)  Venta (L) Llenado dep. almacenam. (L)  \\\n",
       "0                          20300,59      -5785                     7017,89   \n",
       "1                          21522,72      -6040                     7983,27   \n",
       "2                          23302,24      -6005                     4558,82   \n",
       "3                           21823,0      -5820                     5284,44   \n",
       "4                           21257,0      -5975                     6238,65   \n",
       "5                          21639,32      -5895                     7578,84   \n",
       "6                          23420,13      -5820                     4391,04   \n",
       "7                          22048,16      -6135                      7710,2   \n",
       "8                          23568,83      -5700                     3708,58   \n",
       "9                          21671,39      -6240                     6945,83   \n",
       "10                         22417,02      -6635                     8916,91   \n",
       "11                         24664,59      -5835                      3471,0   \n",
       "12                         22355,09      -5905                     5214,81   \n",
       "13                         21648,19      -5865                     5636,35   \n",
       "14                         21467,99      -6170                     4948,56   \n",
       "15                         20227,25      -5945                     7956,88   \n",
       "16                         22267,31      -5780                     5977,77   \n",
       "17                         22511,93      -5895                      5989,0   \n",
       "18                         22574,78      -5760                     6292,65   \n",
       "19                         23200,34      -5935                     3753,77   \n",
       "20                         20963,77      -5490                     6918,55   \n",
       "21                         22352,14      -5670                     6563,68   \n",
       "22                         23206,49      -5805                     5117,12   \n",
       "23                         22516,41      -6315                     8234,24   \n",
       "24                         24424,35      -5860                     3230,48   \n",
       "25                         21783,77      -6315                     5967,93   \n",
       "26                         21343,34      -6070                     5946,98   \n",
       "27                         21269,16      -5900                     6411,31   \n",
       "\n",
       "   Volumen dep. almacenam. fin. teor. (L) Volumen dep. almacenam. fin. (L)  \\\n",
       "0                                21533,48                         21522,72   \n",
       "1                                23465,99                         23302,24   \n",
       "2                                21856,06                          21823,0   \n",
       "3                                21287,44                          21257,0   \n",
       "4                                21520,65                         21639,32   \n",
       "5                                23323,16                         23420,13   \n",
       "6                                21991,17                         22048,16   \n",
       "7                                23623,36                         23568,83   \n",
       "8                                21577,41                         21671,39   \n",
       "9                                22377,22                         22417,02   \n",
       "10                               24698,93                         24664,59   \n",
       "11                               22300,59                         22355,09   \n",
       "12                                21664,9                         21648,19   \n",
       "13                               21419,54                         21467,99   \n",
       "14                               20246,55                         20227,25   \n",
       "15                               22239,13                         22267,31   \n",
       "16                               22465,08                         22511,93   \n",
       "17                               22605,93                         22574,78   \n",
       "18                               23107,43                         23200,34   \n",
       "19                               21019,11                         20963,77   \n",
       "20                               22392,32                         22352,14   \n",
       "21                               23245,82                         23206,49   \n",
       "22                               22518,61                         22516,41   \n",
       "23                               24435,65                         24424,35   \n",
       "24                               21794,83                         21783,77   \n",
       "25                                21436,7                         21343,34   \n",
       "26                               21220,32                         21269,16   \n",
       "27                               21780,47                         21628,04   \n",
       "\n",
       "   Variacion Variacion Acum.  Fugando combustible  \n",
       "0     -10,76           234,6                    0  \n",
       "1    -163,75           35,97                    0  \n",
       "2     -33,06          -38,51                    0  \n",
       "3     -30,44          -45,57                    0  \n",
       "4     118,67           -4,62                    0  \n",
       "5      96,97          105,45                    0  \n",
       "6      56,99          158,48                    0  \n",
       "7     -54,53           48,56                    0  \n",
       "8      93,98          141,14                    0  \n",
       "9       39,8          113,87                    0  \n",
       "10    -34,34           90,29                    0  \n",
       "11      54,5          308,54                    0  \n",
       "12    -16,71          324,89                    0  \n",
       "13     48,45          403,78                    0  \n",
       "14     -19,3          265,81                    1  \n",
       "15     28,18          197,02                    1  \n",
       "16     46,85          186,88                    1  \n",
       "17    -31,15          210,26                    1  \n",
       "18     92,91          209,19                    1  \n",
       "19    -55,34          114,05                    1  \n",
       "20    -40,18          108,21                    1  \n",
       "21    -39,33           14,38                    1  \n",
       "22      -2,2           28,89                    1  \n",
       "23     -11,3          -30,86                    1  \n",
       "24    -11,06          -22,62                    1  \n",
       "25    -93,36         -144,16                    1  \n",
       "26     48,84         -142,17                    1  \n",
       "27   -152,43         -263,45                    1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.read_csv('../../data/datos_simulacion.csv').drop(columns=[\n",
    "    'Unnamed: 0',\n",
    "    'Tiempo (dia)'\n",
    "])\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limpieza de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Volumen dep. almacenam. ini. (L)</th>\n",
       "      <th>Venta (L)</th>\n",
       "      <th>Llenado dep. almacenam. (L)</th>\n",
       "      <th>Volumen dep. almacenam. fin. teor. (L)</th>\n",
       "      <th>Volumen dep. almacenam. fin. (L)</th>\n",
       "      <th>Variacion</th>\n",
       "      <th>Variacion Acum.</th>\n",
       "      <th>Fugando combustible</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20300.59</td>\n",
       "      <td>-5785</td>\n",
       "      <td>7017.89</td>\n",
       "      <td>21533.48</td>\n",
       "      <td>21522.72</td>\n",
       "      <td>-10.76</td>\n",
       "      <td>234.60</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21522.72</td>\n",
       "      <td>-6040</td>\n",
       "      <td>7983.27</td>\n",
       "      <td>23465.99</td>\n",
       "      <td>23302.24</td>\n",
       "      <td>-163.75</td>\n",
       "      <td>35.97</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23302.24</td>\n",
       "      <td>-6005</td>\n",
       "      <td>4558.82</td>\n",
       "      <td>21856.06</td>\n",
       "      <td>21823.00</td>\n",
       "      <td>-33.06</td>\n",
       "      <td>-38.51</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21823.00</td>\n",
       "      <td>-5820</td>\n",
       "      <td>5284.44</td>\n",
       "      <td>21287.44</td>\n",
       "      <td>21257.00</td>\n",
       "      <td>-30.44</td>\n",
       "      <td>-45.57</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21257.00</td>\n",
       "      <td>-5975</td>\n",
       "      <td>6238.65</td>\n",
       "      <td>21520.65</td>\n",
       "      <td>21639.32</td>\n",
       "      <td>118.67</td>\n",
       "      <td>-4.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>21639.32</td>\n",
       "      <td>-5895</td>\n",
       "      <td>7578.84</td>\n",
       "      <td>23323.16</td>\n",
       "      <td>23420.13</td>\n",
       "      <td>96.97</td>\n",
       "      <td>105.45</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>23420.13</td>\n",
       "      <td>-5820</td>\n",
       "      <td>4391.04</td>\n",
       "      <td>21991.17</td>\n",
       "      <td>22048.16</td>\n",
       "      <td>56.99</td>\n",
       "      <td>158.48</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>22048.16</td>\n",
       "      <td>-6135</td>\n",
       "      <td>7710.20</td>\n",
       "      <td>23623.36</td>\n",
       "      <td>23568.83</td>\n",
       "      <td>-54.53</td>\n",
       "      <td>48.56</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>23568.83</td>\n",
       "      <td>-5700</td>\n",
       "      <td>3708.58</td>\n",
       "      <td>21577.41</td>\n",
       "      <td>21671.39</td>\n",
       "      <td>93.98</td>\n",
       "      <td>141.14</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>21671.39</td>\n",
       "      <td>-6240</td>\n",
       "      <td>6945.83</td>\n",
       "      <td>22377.22</td>\n",
       "      <td>22417.02</td>\n",
       "      <td>39.80</td>\n",
       "      <td>113.87</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>22417.02</td>\n",
       "      <td>-6635</td>\n",
       "      <td>8916.91</td>\n",
       "      <td>24698.93</td>\n",
       "      <td>24664.59</td>\n",
       "      <td>-34.34</td>\n",
       "      <td>90.29</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>24664.59</td>\n",
       "      <td>-5835</td>\n",
       "      <td>3471.00</td>\n",
       "      <td>22300.59</td>\n",
       "      <td>22355.09</td>\n",
       "      <td>54.50</td>\n",
       "      <td>308.54</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>22355.09</td>\n",
       "      <td>-5905</td>\n",
       "      <td>5214.81</td>\n",
       "      <td>21664.90</td>\n",
       "      <td>21648.19</td>\n",
       "      <td>-16.71</td>\n",
       "      <td>324.89</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>21648.19</td>\n",
       "      <td>-5865</td>\n",
       "      <td>5636.35</td>\n",
       "      <td>21419.54</td>\n",
       "      <td>21467.99</td>\n",
       "      <td>48.45</td>\n",
       "      <td>403.78</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>21467.99</td>\n",
       "      <td>-6170</td>\n",
       "      <td>4948.56</td>\n",
       "      <td>20246.55</td>\n",
       "      <td>20227.25</td>\n",
       "      <td>-19.30</td>\n",
       "      <td>265.81</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>20227.25</td>\n",
       "      <td>-5945</td>\n",
       "      <td>7956.88</td>\n",
       "      <td>22239.13</td>\n",
       "      <td>22267.31</td>\n",
       "      <td>28.18</td>\n",
       "      <td>197.02</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>22267.31</td>\n",
       "      <td>-5780</td>\n",
       "      <td>5977.77</td>\n",
       "      <td>22465.08</td>\n",
       "      <td>22511.93</td>\n",
       "      <td>46.85</td>\n",
       "      <td>186.88</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>22511.93</td>\n",
       "      <td>-5895</td>\n",
       "      <td>5989.00</td>\n",
       "      <td>22605.93</td>\n",
       "      <td>22574.78</td>\n",
       "      <td>-31.15</td>\n",
       "      <td>210.26</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>22574.78</td>\n",
       "      <td>-5760</td>\n",
       "      <td>6292.65</td>\n",
       "      <td>23107.43</td>\n",
       "      <td>23200.34</td>\n",
       "      <td>92.91</td>\n",
       "      <td>209.19</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>23200.34</td>\n",
       "      <td>-5935</td>\n",
       "      <td>3753.77</td>\n",
       "      <td>21019.11</td>\n",
       "      <td>20963.77</td>\n",
       "      <td>-55.34</td>\n",
       "      <td>114.05</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20963.77</td>\n",
       "      <td>-5490</td>\n",
       "      <td>6918.55</td>\n",
       "      <td>22392.32</td>\n",
       "      <td>22352.14</td>\n",
       "      <td>-40.18</td>\n",
       "      <td>108.21</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22352.14</td>\n",
       "      <td>-5670</td>\n",
       "      <td>6563.68</td>\n",
       "      <td>23245.82</td>\n",
       "      <td>23206.49</td>\n",
       "      <td>-39.33</td>\n",
       "      <td>14.38</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23206.49</td>\n",
       "      <td>-5805</td>\n",
       "      <td>5117.12</td>\n",
       "      <td>22518.61</td>\n",
       "      <td>22516.41</td>\n",
       "      <td>-2.20</td>\n",
       "      <td>28.89</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>22516.41</td>\n",
       "      <td>-6315</td>\n",
       "      <td>8234.24</td>\n",
       "      <td>24435.65</td>\n",
       "      <td>24424.35</td>\n",
       "      <td>-11.30</td>\n",
       "      <td>-30.86</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24424.35</td>\n",
       "      <td>-5860</td>\n",
       "      <td>3230.48</td>\n",
       "      <td>21794.83</td>\n",
       "      <td>21783.77</td>\n",
       "      <td>-11.06</td>\n",
       "      <td>-22.62</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>21783.77</td>\n",
       "      <td>-6315</td>\n",
       "      <td>5967.93</td>\n",
       "      <td>21436.70</td>\n",
       "      <td>21343.34</td>\n",
       "      <td>-93.36</td>\n",
       "      <td>-144.16</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>21343.34</td>\n",
       "      <td>-6070</td>\n",
       "      <td>5946.98</td>\n",
       "      <td>21220.32</td>\n",
       "      <td>21269.16</td>\n",
       "      <td>48.84</td>\n",
       "      <td>-142.17</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>21269.16</td>\n",
       "      <td>-5900</td>\n",
       "      <td>6411.31</td>\n",
       "      <td>21780.47</td>\n",
       "      <td>21628.04</td>\n",
       "      <td>-152.43</td>\n",
       "      <td>-263.45</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Volumen dep. almacenam. ini. (L)  Venta (L)  Llenado dep. almacenam. (L)  \\\n",
       "0                           20300.59      -5785                      7017.89   \n",
       "1                           21522.72      -6040                      7983.27   \n",
       "2                           23302.24      -6005                      4558.82   \n",
       "3                           21823.00      -5820                      5284.44   \n",
       "4                           21257.00      -5975                      6238.65   \n",
       "5                           21639.32      -5895                      7578.84   \n",
       "6                           23420.13      -5820                      4391.04   \n",
       "7                           22048.16      -6135                      7710.20   \n",
       "8                           23568.83      -5700                      3708.58   \n",
       "9                           21671.39      -6240                      6945.83   \n",
       "10                          22417.02      -6635                      8916.91   \n",
       "11                          24664.59      -5835                      3471.00   \n",
       "12                          22355.09      -5905                      5214.81   \n",
       "13                          21648.19      -5865                      5636.35   \n",
       "14                          21467.99      -6170                      4948.56   \n",
       "15                          20227.25      -5945                      7956.88   \n",
       "16                          22267.31      -5780                      5977.77   \n",
       "17                          22511.93      -5895                      5989.00   \n",
       "18                          22574.78      -5760                      6292.65   \n",
       "19                          23200.34      -5935                      3753.77   \n",
       "20                          20963.77      -5490                      6918.55   \n",
       "21                          22352.14      -5670                      6563.68   \n",
       "22                          23206.49      -5805                      5117.12   \n",
       "23                          22516.41      -6315                      8234.24   \n",
       "24                          24424.35      -5860                      3230.48   \n",
       "25                          21783.77      -6315                      5967.93   \n",
       "26                          21343.34      -6070                      5946.98   \n",
       "27                          21269.16      -5900                      6411.31   \n",
       "\n",
       "    Volumen dep. almacenam. fin. teor. (L)  Volumen dep. almacenam. fin. (L)  \\\n",
       "0                                 21533.48                          21522.72   \n",
       "1                                 23465.99                          23302.24   \n",
       "2                                 21856.06                          21823.00   \n",
       "3                                 21287.44                          21257.00   \n",
       "4                                 21520.65                          21639.32   \n",
       "5                                 23323.16                          23420.13   \n",
       "6                                 21991.17                          22048.16   \n",
       "7                                 23623.36                          23568.83   \n",
       "8                                 21577.41                          21671.39   \n",
       "9                                 22377.22                          22417.02   \n",
       "10                                24698.93                          24664.59   \n",
       "11                                22300.59                          22355.09   \n",
       "12                                21664.90                          21648.19   \n",
       "13                                21419.54                          21467.99   \n",
       "14                                20246.55                          20227.25   \n",
       "15                                22239.13                          22267.31   \n",
       "16                                22465.08                          22511.93   \n",
       "17                                22605.93                          22574.78   \n",
       "18                                23107.43                          23200.34   \n",
       "19                                21019.11                          20963.77   \n",
       "20                                22392.32                          22352.14   \n",
       "21                                23245.82                          23206.49   \n",
       "22                                22518.61                          22516.41   \n",
       "23                                24435.65                          24424.35   \n",
       "24                                21794.83                          21783.77   \n",
       "25                                21436.70                          21343.34   \n",
       "26                                21220.32                          21269.16   \n",
       "27                                21780.47                          21628.04   \n",
       "\n",
       "    Variacion  Variacion Acum.  Fugando combustible  \n",
       "0      -10.76           234.60                    0  \n",
       "1     -163.75            35.97                    0  \n",
       "2      -33.06           -38.51                    0  \n",
       "3      -30.44           -45.57                    0  \n",
       "4      118.67            -4.62                    0  \n",
       "5       96.97           105.45                    0  \n",
       "6       56.99           158.48                    0  \n",
       "7      -54.53            48.56                    0  \n",
       "8       93.98           141.14                    0  \n",
       "9       39.80           113.87                    0  \n",
       "10     -34.34            90.29                    0  \n",
       "11      54.50           308.54                    0  \n",
       "12     -16.71           324.89                    0  \n",
       "13      48.45           403.78                    0  \n",
       "14     -19.30           265.81                    1  \n",
       "15      28.18           197.02                    1  \n",
       "16      46.85           186.88                    1  \n",
       "17     -31.15           210.26                    1  \n",
       "18      92.91           209.19                    1  \n",
       "19     -55.34           114.05                    1  \n",
       "20     -40.18           108.21                    1  \n",
       "21     -39.33            14.38                    1  \n",
       "22      -2.20            28.89                    1  \n",
       "23     -11.30           -30.86                    1  \n",
       "24     -11.06           -22.62                    1  \n",
       "25     -93.36          -144.16                    1  \n",
       "26      48.84          -142.17                    1  \n",
       "27    -152.43          -263.45                    1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bad_formatted_fields = [\n",
    "    'Volumen dep. almacenam. ini. (L)',\n",
    "    'Llenado dep. almacenam. (L)',\n",
    "    'Volumen dep. almacenam. fin. teor. (L)',\n",
    "    'Volumen dep. almacenam. fin. (L)',\n",
    "    'Variacion',\n",
    "    'Variacion Acum.',\n",
    "]\n",
    "\n",
    "for column in bad_formatted_fields:\n",
    "    df[column] = df[column].str.replace(',', '.')\n",
    "    df[column] = pd.to_numeric(df[column])\n",
    "\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descripción de campos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- TODO -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesados de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Volumen dep. almacenam. ini. (L)</th>\n",
       "      <th>Venta (L)</th>\n",
       "      <th>Llenado dep. almacenam. (L)</th>\n",
       "      <th>Volumen dep. almacenam. fin. teor. (L)</th>\n",
       "      <th>Volumen dep. almacenam. fin. (L)</th>\n",
       "      <th>Variacion</th>\n",
       "      <th>Variacion Acum.</th>\n",
       "      <th>Fugando combustible</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.016528</td>\n",
       "      <td>-5785</td>\n",
       "      <td>0.666044</td>\n",
       "      <td>0.289043</td>\n",
       "      <td>0.291947</td>\n",
       "      <td>0.541711</td>\n",
       "      <td>0.746444</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.291947</td>\n",
       "      <td>-6040</td>\n",
       "      <td>0.835813</td>\n",
       "      <td>0.723083</td>\n",
       "      <td>0.692980</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.448751</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.692980</td>\n",
       "      <td>-6005</td>\n",
       "      <td>0.233598</td>\n",
       "      <td>0.361494</td>\n",
       "      <td>0.359619</td>\n",
       "      <td>0.462751</td>\n",
       "      <td>0.337125</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.359619</td>\n",
       "      <td>-5820</td>\n",
       "      <td>0.361204</td>\n",
       "      <td>0.233783</td>\n",
       "      <td>0.232065</td>\n",
       "      <td>0.472027</td>\n",
       "      <td>0.326544</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.232065</td>\n",
       "      <td>-5975</td>\n",
       "      <td>0.529009</td>\n",
       "      <td>0.286162</td>\n",
       "      <td>0.318224</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.387917</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.318224</td>\n",
       "      <td>-5895</td>\n",
       "      <td>0.764691</td>\n",
       "      <td>0.691003</td>\n",
       "      <td>0.719548</td>\n",
       "      <td>0.923164</td>\n",
       "      <td>0.552883</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.719548</td>\n",
       "      <td>-5820</td>\n",
       "      <td>0.204093</td>\n",
       "      <td>0.391840</td>\n",
       "      <td>0.410361</td>\n",
       "      <td>0.781602</td>\n",
       "      <td>0.632361</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.410361</td>\n",
       "      <td>-6135</td>\n",
       "      <td>0.787791</td>\n",
       "      <td>0.758428</td>\n",
       "      <td>0.753059</td>\n",
       "      <td>0.386729</td>\n",
       "      <td>0.467620</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.753059</td>\n",
       "      <td>-5700</td>\n",
       "      <td>0.084077</td>\n",
       "      <td>0.298910</td>\n",
       "      <td>0.325452</td>\n",
       "      <td>0.912577</td>\n",
       "      <td>0.606373</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.325452</td>\n",
       "      <td>-6240</td>\n",
       "      <td>0.653371</td>\n",
       "      <td>0.478546</td>\n",
       "      <td>0.493487</td>\n",
       "      <td>0.720735</td>\n",
       "      <td>0.565502</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.493487</td>\n",
       "      <td>-6635</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.458218</td>\n",
       "      <td>0.530162</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-5835</td>\n",
       "      <td>0.042297</td>\n",
       "      <td>0.461335</td>\n",
       "      <td>0.479531</td>\n",
       "      <td>0.772785</td>\n",
       "      <td>0.857261</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.479531</td>\n",
       "      <td>-5905</td>\n",
       "      <td>0.348959</td>\n",
       "      <td>0.318560</td>\n",
       "      <td>0.320223</td>\n",
       "      <td>0.520643</td>\n",
       "      <td>0.881765</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.320223</td>\n",
       "      <td>-5865</td>\n",
       "      <td>0.423090</td>\n",
       "      <td>0.263452</td>\n",
       "      <td>0.279613</td>\n",
       "      <td>0.751363</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.279613</td>\n",
       "      <td>-6170</td>\n",
       "      <td>0.302137</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.511472</td>\n",
       "      <td>0.793220</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5945</td>\n",
       "      <td>0.831172</td>\n",
       "      <td>0.447531</td>\n",
       "      <td>0.459748</td>\n",
       "      <td>0.679591</td>\n",
       "      <td>0.690122</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.459748</td>\n",
       "      <td>-5780</td>\n",
       "      <td>0.483131</td>\n",
       "      <td>0.498280</td>\n",
       "      <td>0.514876</td>\n",
       "      <td>0.745698</td>\n",
       "      <td>0.674925</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.514876</td>\n",
       "      <td>-5895</td>\n",
       "      <td>0.485106</td>\n",
       "      <td>0.529914</td>\n",
       "      <td>0.529040</td>\n",
       "      <td>0.469513</td>\n",
       "      <td>0.709965</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.529040</td>\n",
       "      <td>-5760</td>\n",
       "      <td>0.538505</td>\n",
       "      <td>0.642551</td>\n",
       "      <td>0.670016</td>\n",
       "      <td>0.908788</td>\n",
       "      <td>0.708361</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.670016</td>\n",
       "      <td>-5935</td>\n",
       "      <td>0.092024</td>\n",
       "      <td>0.173516</td>\n",
       "      <td>0.165982</td>\n",
       "      <td>0.383861</td>\n",
       "      <td>0.565772</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.165982</td>\n",
       "      <td>-5490</td>\n",
       "      <td>0.648574</td>\n",
       "      <td>0.481938</td>\n",
       "      <td>0.478866</td>\n",
       "      <td>0.437540</td>\n",
       "      <td>0.557019</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.478866</td>\n",
       "      <td>-5670</td>\n",
       "      <td>0.586167</td>\n",
       "      <td>0.673633</td>\n",
       "      <td>0.671402</td>\n",
       "      <td>0.440550</td>\n",
       "      <td>0.416393</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.671402</td>\n",
       "      <td>-5805</td>\n",
       "      <td>0.331779</td>\n",
       "      <td>0.510302</td>\n",
       "      <td>0.515886</td>\n",
       "      <td>0.572020</td>\n",
       "      <td>0.438140</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.515886</td>\n",
       "      <td>-6315</td>\n",
       "      <td>0.879948</td>\n",
       "      <td>0.940868</td>\n",
       "      <td>0.945859</td>\n",
       "      <td>0.539799</td>\n",
       "      <td>0.348590</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.945859</td>\n",
       "      <td>-5860</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.347742</td>\n",
       "      <td>0.350778</td>\n",
       "      <td>0.540649</td>\n",
       "      <td>0.360940</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.350778</td>\n",
       "      <td>-6315</td>\n",
       "      <td>0.481400</td>\n",
       "      <td>0.267306</td>\n",
       "      <td>0.251522</td>\n",
       "      <td>0.249239</td>\n",
       "      <td>0.178784</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.251522</td>\n",
       "      <td>-6070</td>\n",
       "      <td>0.477716</td>\n",
       "      <td>0.218708</td>\n",
       "      <td>0.234805</td>\n",
       "      <td>0.752744</td>\n",
       "      <td>0.181766</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.234805</td>\n",
       "      <td>-5900</td>\n",
       "      <td>0.559372</td>\n",
       "      <td>0.344517</td>\n",
       "      <td>0.315682</td>\n",
       "      <td>0.040082</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Volumen dep. almacenam. ini. (L)  Venta (L)  Llenado dep. almacenam. (L)  \\\n",
       "0                           0.016528      -5785                     0.666044   \n",
       "1                           0.291947      -6040                     0.835813   \n",
       "2                           0.692980      -6005                     0.233598   \n",
       "3                           0.359619      -5820                     0.361204   \n",
       "4                           0.232065      -5975                     0.529009   \n",
       "5                           0.318224      -5895                     0.764691   \n",
       "6                           0.719548      -5820                     0.204093   \n",
       "7                           0.410361      -6135                     0.787791   \n",
       "8                           0.753059      -5700                     0.084077   \n",
       "9                           0.325452      -6240                     0.653371   \n",
       "10                          0.493487      -6635                     1.000000   \n",
       "11                          1.000000      -5835                     0.042297   \n",
       "12                          0.479531      -5905                     0.348959   \n",
       "13                          0.320223      -5865                     0.423090   \n",
       "14                          0.279613      -6170                     0.302137   \n",
       "15                          0.000000      -5945                     0.831172   \n",
       "16                          0.459748      -5780                     0.483131   \n",
       "17                          0.514876      -5895                     0.485106   \n",
       "18                          0.529040      -5760                     0.538505   \n",
       "19                          0.670016      -5935                     0.092024   \n",
       "20                          0.165982      -5490                     0.648574   \n",
       "21                          0.478866      -5670                     0.586167   \n",
       "22                          0.671402      -5805                     0.331779   \n",
       "23                          0.515886      -6315                     0.879948   \n",
       "24                          0.945859      -5860                     0.000000   \n",
       "25                          0.350778      -6315                     0.481400   \n",
       "26                          0.251522      -6070                     0.477716   \n",
       "27                          0.234805      -5900                     0.559372   \n",
       "\n",
       "    Volumen dep. almacenam. fin. teor. (L)  Volumen dep. almacenam. fin. (L)  \\\n",
       "0                                 0.289043                          0.291947   \n",
       "1                                 0.723083                          0.692980   \n",
       "2                                 0.361494                          0.359619   \n",
       "3                                 0.233783                          0.232065   \n",
       "4                                 0.286162                          0.318224   \n",
       "5                                 0.691003                          0.719548   \n",
       "6                                 0.391840                          0.410361   \n",
       "7                                 0.758428                          0.753059   \n",
       "8                                 0.298910                          0.325452   \n",
       "9                                 0.478546                          0.493487   \n",
       "10                                1.000000                          1.000000   \n",
       "11                                0.461335                          0.479531   \n",
       "12                                0.318560                          0.320223   \n",
       "13                                0.263452                          0.279613   \n",
       "14                                0.000000                          0.000000   \n",
       "15                                0.447531                          0.459748   \n",
       "16                                0.498280                          0.514876   \n",
       "17                                0.529914                          0.529040   \n",
       "18                                0.642551                          0.670016   \n",
       "19                                0.173516                          0.165982   \n",
       "20                                0.481938                          0.478866   \n",
       "21                                0.673633                          0.671402   \n",
       "22                                0.510302                          0.515886   \n",
       "23                                0.940868                          0.945859   \n",
       "24                                0.347742                          0.350778   \n",
       "25                                0.267306                          0.251522   \n",
       "26                                0.218708                          0.234805   \n",
       "27                                0.344517                          0.315682   \n",
       "\n",
       "    Variacion  Variacion Acum.  Fugando combustible  \n",
       "0    0.541711         0.746444                    0  \n",
       "1    0.000000         0.448751                    0  \n",
       "2    0.462751         0.337125                    0  \n",
       "3    0.472027         0.326544                    0  \n",
       "4    1.000000         0.387917                    0  \n",
       "5    0.923164         0.552883                    0  \n",
       "6    0.781602         0.632361                    0  \n",
       "7    0.386729         0.467620                    0  \n",
       "8    0.912577         0.606373                    0  \n",
       "9    0.720735         0.565502                    0  \n",
       "10   0.458218         0.530162                    0  \n",
       "11   0.772785         0.857261                    0  \n",
       "12   0.520643         0.881765                    0  \n",
       "13   0.751363         1.000000                    0  \n",
       "14   0.511472         0.793220                    1  \n",
       "15   0.679591         0.690122                    1  \n",
       "16   0.745698         0.674925                    1  \n",
       "17   0.469513         0.709965                    1  \n",
       "18   0.908788         0.708361                    1  \n",
       "19   0.383861         0.565772                    1  \n",
       "20   0.437540         0.557019                    1  \n",
       "21   0.440550         0.416393                    1  \n",
       "22   0.572020         0.438140                    1  \n",
       "23   0.539799         0.348590                    1  \n",
       "24   0.540649         0.360940                    1  \n",
       "25   0.249239         0.178784                    1  \n",
       "26   0.752744         0.181766                    1  \n",
       "27   0.040082         0.000000                    1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "numeric_cols = [\n",
    "    'Volumen dep. almacenam. ini. (L)',\n",
    "    'Llenado dep. almacenam. (L)',\n",
    "    'Volumen dep. almacenam. fin. teor. (L)',\n",
    "    'Volumen dep. almacenam. fin. (L)',\n",
    "    'Variacion',\n",
    "    'Variacion Acum.',\n",
    "]\n",
    "\n",
    "for col in numeric_cols:\n",
    "    df[col] = MinMaxScaler().fit_transform(df[[col]])\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementación de modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_13\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_13\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,020</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm_5 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m)             │         \u001b[38;5;34m1,020\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_22 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m16\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,036</span> (4.05 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,036\u001b[0m (4.05 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,036</span> (4.05 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,036\u001b[0m (4.05 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "OBJECTIVE_VARIABLE = 'Fugando combustible'\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df.drop(columns=OBJECTIVE_VARIABLE),\n",
    "    df[OBJECTIVE_VARIABLE],\n",
    "    stratify=df[OBJECTIVE_VARIABLE]\n",
    ")\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(X_train.shape[1], 1)),\n",
    "    tf.keras.layers.LSTM(15),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 0.0041 - val_accuracy: 0.7143 - val_loss: 1.5681\n",
      "Epoch 2/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 0.0061 - val_accuracy: 0.7143 - val_loss: 1.5955\n",
      "Epoch 3/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 1.0000 - loss: 0.0046 - val_accuracy: 0.7143 - val_loss: 1.5389\n",
      "Epoch 4/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 0.0067 - val_accuracy: 0.7143 - val_loss: 1.4060\n",
      "Epoch 5/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 0.0044 - val_accuracy: 0.7143 - val_loss: 1.3597\n",
      "Epoch 6/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 0.0043 - val_accuracy: 0.7143 - val_loss: 1.4009\n",
      "Epoch 7/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 0.0050 - val_accuracy: 0.7143 - val_loss: 1.4935\n",
      "Epoch 8/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 0.0050 - val_accuracy: 0.7143 - val_loss: 1.5948\n",
      "Epoch 9/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 0.0043 - val_accuracy: 0.7143 - val_loss: 1.6703\n",
      "Epoch 10/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 0.0040 - val_accuracy: 0.7143 - val_loss: 1.6988\n",
      "Epoch 11/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 0.0042 - val_accuracy: 0.7143 - val_loss: 1.6782\n",
      "Epoch 12/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 0.0044 - val_accuracy: 0.7143 - val_loss: 1.6198\n",
      "Epoch 13/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 0.0042 - val_accuracy: 0.7143 - val_loss: 1.5424\n",
      "Epoch 14/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 1.0000 - loss: 0.0038 - val_accuracy: 0.7143 - val_loss: 1.4675\n",
      "Epoch 15/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 0.0037 - val_accuracy: 0.7143 - val_loss: 1.4160\n",
      "Epoch 16/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 0.0038 - val_accuracy: 0.7143 - val_loss: 1.4029\n",
      "Epoch 17/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 0.0040 - val_accuracy: 0.7143 - val_loss: 1.4296\n",
      "Epoch 18/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 0.0039 - val_accuracy: 0.7143 - val_loss: 1.4845\n",
      "Epoch 19/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 1.0000 - loss: 0.0037 - val_accuracy: 0.7143 - val_loss: 1.5513\n",
      "Epoch 20/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 1.0000 - loss: 0.0035 - val_accuracy: 0.7143 - val_loss: 1.6144\n",
      "Epoch 21/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 0.0035 - val_accuracy: 0.7143 - val_loss: 1.6603\n",
      "Epoch 22/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 0.0036 - val_accuracy: 0.7143 - val_loss: 1.6803\n",
      "Epoch 23/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 0.0037 - val_accuracy: 0.7143 - val_loss: 1.6722\n",
      "Epoch 24/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 0.0036 - val_accuracy: 0.7143 - val_loss: 1.6415\n",
      "Epoch 25/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 0.0034 - val_accuracy: 0.7143 - val_loss: 1.5981\n",
      "Epoch 26/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 0.0033 - val_accuracy: 0.7143 - val_loss: 1.5531\n",
      "Epoch 27/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 0.0033 - val_accuracy: 0.7143 - val_loss: 1.5171\n",
      "Epoch 28/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 0.0034 - val_accuracy: 0.7143 - val_loss: 1.4983\n",
      "Epoch 29/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 0.0034 - val_accuracy: 0.7143 - val_loss: 1.5003\n",
      "Epoch 30/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 0.0033 - val_accuracy: 0.7143 - val_loss: 1.5206\n",
      "Epoch 31/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 0.0032 - val_accuracy: 0.7143 - val_loss: 1.5524\n",
      "Epoch 32/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 1.0000 - loss: 0.0032 - val_accuracy: 0.7143 - val_loss: 1.5873\n",
      "Epoch 33/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 1.0000 - loss: 0.0032 - val_accuracy: 0.7143 - val_loss: 1.6174\n",
      "Epoch 34/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 0.0032 - val_accuracy: 0.7143 - val_loss: 1.6367\n",
      "Epoch 35/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 0.0032 - val_accuracy: 0.7143 - val_loss: 1.6428\n",
      "Epoch 36/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 0.0031 - val_accuracy: 0.7143 - val_loss: 1.6366\n",
      "Epoch 37/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 0.0031 - val_accuracy: 0.7143 - val_loss: 1.6216\n",
      "Epoch 38/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 0.0030 - val_accuracy: 0.7143 - val_loss: 1.6030\n",
      "Epoch 39/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 0.0030 - val_accuracy: 0.7143 - val_loss: 1.5861\n",
      "Epoch 40/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 1.0000 - loss: 0.0030 - val_accuracy: 0.7143 - val_loss: 1.5751\n",
      "Epoch 41/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 0.0030 - val_accuracy: 0.7143 - val_loss: 1.5723\n",
      "Epoch 42/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 0.0029 - val_accuracy: 0.7143 - val_loss: 1.5776\n",
      "Epoch 43/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 0.0029 - val_accuracy: 0.7143 - val_loss: 1.5888\n",
      "Epoch 44/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 0.0029 - val_accuracy: 0.7143 - val_loss: 1.6027\n",
      "Epoch 45/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 0.0029 - val_accuracy: 0.7143 - val_loss: 1.6160\n",
      "Epoch 46/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 1.0000 - loss: 0.0029 - val_accuracy: 0.7143 - val_loss: 1.6267\n",
      "Epoch 47/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 0.0028 - val_accuracy: 0.7143 - val_loss: 1.6336\n",
      "Epoch 48/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 0.0028 - val_accuracy: 0.7143 - val_loss: 1.6369\n",
      "Epoch 49/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 0.0028 - val_accuracy: 0.7143 - val_loss: 1.6373\n",
      "Epoch 50/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 0.0028 - val_accuracy: 0.7143 - val_loss: 1.6360\n",
      "Epoch 51/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 0.0027 - val_accuracy: 0.7143 - val_loss: 1.6342\n",
      "Epoch 52/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 0.0027 - val_accuracy: 0.7143 - val_loss: 1.6329\n",
      "Epoch 53/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 0.0027 - val_accuracy: 0.7143 - val_loss: 1.6325\n",
      "Epoch 54/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 1.0000 - loss: 0.0027 - val_accuracy: 0.7143 - val_loss: 1.6332\n",
      "Epoch 55/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 0.0026 - val_accuracy: 0.7143 - val_loss: 1.6348\n",
      "Epoch 56/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 0.0026 - val_accuracy: 0.7143 - val_loss: 1.6370\n",
      "Epoch 57/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 0.0026 - val_accuracy: 0.7143 - val_loss: 1.6400\n",
      "Epoch 58/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 1.0000 - loss: 0.0026 - val_accuracy: 0.7143 - val_loss: 1.6439\n",
      "Epoch 59/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 0.0026 - val_accuracy: 0.7143 - val_loss: 1.6488\n",
      "Epoch 60/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 0.0025 - val_accuracy: 0.7143 - val_loss: 1.6546\n",
      "Epoch 61/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 0.0025 - val_accuracy: 0.7143 - val_loss: 1.6606\n",
      "Epoch 62/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 0.0025 - val_accuracy: 0.7143 - val_loss: 1.6663\n",
      "Epoch 63/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 0.0025 - val_accuracy: 0.7143 - val_loss: 1.6708\n",
      "Epoch 64/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 0.0025 - val_accuracy: 0.7143 - val_loss: 1.6735\n",
      "Epoch 65/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 1.0000 - loss: 0.0024 - val_accuracy: 0.7143 - val_loss: 1.6743\n",
      "Epoch 66/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 0.0024 - val_accuracy: 0.7143 - val_loss: 1.6735\n",
      "Epoch 67/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 1.0000 - loss: 0.0024 - val_accuracy: 0.7143 - val_loss: 1.6718\n",
      "Epoch 68/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 0.0024 - val_accuracy: 0.7143 - val_loss: 1.6705\n",
      "Epoch 69/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 0.0024 - val_accuracy: 0.7143 - val_loss: 1.6705\n",
      "Epoch 70/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 0.0024 - val_accuracy: 0.7143 - val_loss: 1.6728\n",
      "Epoch 71/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 1.0000 - loss: 0.0023 - val_accuracy: 0.7143 - val_loss: 1.6773\n",
      "Epoch 72/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 1.0000 - loss: 0.0023 - val_accuracy: 0.7143 - val_loss: 1.6837\n",
      "Epoch 73/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 0.0023 - val_accuracy: 0.7143 - val_loss: 1.6907\n",
      "Epoch 74/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 1.0000 - loss: 0.0023 - val_accuracy: 0.7143 - val_loss: 1.6972\n",
      "Epoch 75/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 1.0000 - loss: 0.0023 - val_accuracy: 0.7143 - val_loss: 1.7020\n",
      "Epoch 76/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 1.0000 - loss: 0.0023 - val_accuracy: 0.7143 - val_loss: 1.7046\n",
      "Epoch 77/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 1.0000 - loss: 0.0022 - val_accuracy: 0.7143 - val_loss: 1.7050\n",
      "Epoch 78/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 0.0022 - val_accuracy: 0.7143 - val_loss: 1.7039\n",
      "Epoch 79/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 0.0022 - val_accuracy: 0.7143 - val_loss: 1.7024\n",
      "Epoch 80/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 0.0022 - val_accuracy: 0.7143 - val_loss: 1.7018\n",
      "Epoch 81/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 0.0022 - val_accuracy: 0.7143 - val_loss: 1.7029\n",
      "Epoch 82/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 0.0022 - val_accuracy: 0.7143 - val_loss: 1.7061\n",
      "Epoch 83/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 1.0000 - loss: 0.0021 - val_accuracy: 0.7143 - val_loss: 1.7111\n",
      "Epoch 84/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 0.0021 - val_accuracy: 0.7143 - val_loss: 1.7171\n",
      "Epoch 85/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 0.0021 - val_accuracy: 0.7143 - val_loss: 1.7232\n",
      "Epoch 86/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 0.0021 - val_accuracy: 0.7143 - val_loss: 1.7283\n",
      "Epoch 87/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 1.0000 - loss: 0.0021 - val_accuracy: 0.7143 - val_loss: 1.7318\n",
      "Epoch 88/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 1.0000 - loss: 0.0021 - val_accuracy: 0.7143 - val_loss: 1.7336\n",
      "Epoch 89/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 0.0020 - val_accuracy: 0.7143 - val_loss: 1.7340\n",
      "Epoch 90/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 0.0020 - val_accuracy: 0.7143 - val_loss: 1.7340\n",
      "Epoch 91/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 0.0020 - val_accuracy: 0.7143 - val_loss: 1.7342\n",
      "Epoch 92/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 0.0020 - val_accuracy: 0.7143 - val_loss: 1.7354\n",
      "Epoch 93/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 0.0020 - val_accuracy: 0.7143 - val_loss: 1.7379\n",
      "Epoch 94/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 0.0020 - val_accuracy: 0.7143 - val_loss: 1.7416\n",
      "Epoch 95/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 0.0020 - val_accuracy: 0.7143 - val_loss: 1.7462\n",
      "Epoch 96/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 0.0019 - val_accuracy: 0.7143 - val_loss: 1.7509\n",
      "Epoch 97/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 0.0019 - val_accuracy: 0.7143 - val_loss: 1.7552\n",
      "Epoch 98/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 1.0000 - loss: 0.0019 - val_accuracy: 0.7143 - val_loss: 1.7585\n",
      "Epoch 99/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 1.0000 - loss: 0.0019 - val_accuracy: 0.7143 - val_loss: 1.7609\n",
      "Epoch 100/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 1.0000 - loss: 0.0019 - val_accuracy: 0.7143 - val_loss: 1.7625\n",
      "Epoch 101/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 1.0000 - loss: 0.0019 - val_accuracy: 0.7143 - val_loss: 1.7637\n",
      "Epoch 102/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 1.0000 - loss: 0.0019 - val_accuracy: 0.7143 - val_loss: 1.7649\n",
      "Epoch 103/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 1.0000 - loss: 0.0019 - val_accuracy: 0.7143 - val_loss: 1.7667\n",
      "Epoch 104/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 1.0000 - loss: 0.0018 - val_accuracy: 0.7143 - val_loss: 1.7691\n",
      "Epoch 105/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 1.0000 - loss: 0.0018 - val_accuracy: 0.7143 - val_loss: 1.7721\n",
      "Epoch 106/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 0.0018 - val_accuracy: 0.7143 - val_loss: 1.7756\n",
      "Epoch 107/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 0.0018 - val_accuracy: 0.7143 - val_loss: 1.7792\n",
      "Epoch 108/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 0.0018 - val_accuracy: 0.7143 - val_loss: 1.7826\n",
      "Epoch 109/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 0.0018 - val_accuracy: 0.7143 - val_loss: 1.7856\n",
      "Epoch 110/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 0.0018 - val_accuracy: 0.7143 - val_loss: 1.7881\n",
      "Epoch 111/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 0.0018 - val_accuracy: 0.7143 - val_loss: 1.7902\n",
      "Epoch 112/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 0.0017 - val_accuracy: 0.7143 - val_loss: 1.7921\n",
      "Epoch 113/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 0.0017 - val_accuracy: 0.7143 - val_loss: 1.7941\n",
      "Epoch 114/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 0.0017 - val_accuracy: 0.7143 - val_loss: 1.7963\n",
      "Epoch 115/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 0.0017 - val_accuracy: 0.7143 - val_loss: 1.7988\n",
      "Epoch 116/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 0.0017 - val_accuracy: 0.7143 - val_loss: 1.8015\n",
      "Epoch 117/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 1.0000 - loss: 0.0017 - val_accuracy: 0.7143 - val_loss: 1.8044\n",
      "Epoch 118/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 0.0017 - val_accuracy: 0.7143 - val_loss: 1.8073\n",
      "Epoch 119/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 1.0000 - loss: 0.0017 - val_accuracy: 0.7143 - val_loss: 1.8101\n",
      "Epoch 120/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 0.0017 - val_accuracy: 0.7143 - val_loss: 1.8127\n",
      "Epoch 121/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 1.0000 - loss: 0.0016 - val_accuracy: 0.7143 - val_loss: 1.8152\n",
      "Epoch 122/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 0.0016 - val_accuracy: 0.7143 - val_loss: 1.8176\n",
      "Epoch 123/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 1.0000 - loss: 0.0016 - val_accuracy: 0.7143 - val_loss: 1.8199\n",
      "Epoch 124/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 1.0000 - loss: 0.0016 - val_accuracy: 0.7143 - val_loss: 1.8223\n",
      "Epoch 125/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 1.0000 - loss: 0.0016 - val_accuracy: 0.7143 - val_loss: 1.8248\n",
      "Epoch 126/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 0.0016 - val_accuracy: 0.7143 - val_loss: 1.8274\n",
      "Epoch 127/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 0.0016 - val_accuracy: 0.7143 - val_loss: 1.8300\n",
      "Epoch 128/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 1.0000 - loss: 0.0016 - val_accuracy: 0.7143 - val_loss: 1.8326\n",
      "Epoch 129/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 1.0000 - loss: 0.0016 - val_accuracy: 0.7143 - val_loss: 1.8351\n",
      "Epoch 130/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 1.0000 - loss: 0.0016 - val_accuracy: 0.7143 - val_loss: 1.8376\n",
      "Epoch 131/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 1.0000 - loss: 0.0015 - val_accuracy: 0.7143 - val_loss: 1.8401\n",
      "Epoch 132/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 0.0015 - val_accuracy: 0.7143 - val_loss: 1.8425\n",
      "Epoch 133/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 0.0015 - val_accuracy: 0.7143 - val_loss: 1.8449\n",
      "Epoch 134/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 0.0015 - val_accuracy: 0.7143 - val_loss: 1.8474\n",
      "Epoch 135/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 1.0000 - loss: 0.0015 - val_accuracy: 0.7143 - val_loss: 1.8499\n",
      "Epoch 136/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 1.0000 - loss: 0.0015 - val_accuracy: 0.7143 - val_loss: 1.8525\n",
      "Epoch 137/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 0.0015 - val_accuracy: 0.7143 - val_loss: 1.8550\n",
      "Epoch 138/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 0.0015 - val_accuracy: 0.7143 - val_loss: 1.8576\n",
      "Epoch 139/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 0.0015 - val_accuracy: 0.7143 - val_loss: 1.8601\n",
      "Epoch 140/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 1.0000 - loss: 0.0015 - val_accuracy: 0.7143 - val_loss: 1.8625\n",
      "Epoch 141/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 0.0015 - val_accuracy: 0.7143 - val_loss: 1.8648\n",
      "Epoch 142/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 0.0014 - val_accuracy: 0.7143 - val_loss: 1.8671\n",
      "Epoch 143/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 1.0000 - loss: 0.0014 - val_accuracy: 0.7143 - val_loss: 1.8695\n",
      "Epoch 144/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 1.0000 - loss: 0.0014 - val_accuracy: 0.7143 - val_loss: 1.8719\n",
      "Epoch 145/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 0.0014 - val_accuracy: 0.7143 - val_loss: 1.8743\n",
      "Epoch 146/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 0.0014 - val_accuracy: 0.7143 - val_loss: 1.8768\n",
      "Epoch 147/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 0.0014 - val_accuracy: 0.7143 - val_loss: 1.8793\n",
      "Epoch 148/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 0.0014 - val_accuracy: 0.7143 - val_loss: 1.8819\n",
      "Epoch 149/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 0.0014 - val_accuracy: 0.7143 - val_loss: 1.8843\n",
      "Epoch 150/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 0.0014 - val_accuracy: 0.7143 - val_loss: 1.8867\n",
      "Epoch 151/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 0.0014 - val_accuracy: 0.7143 - val_loss: 1.8890\n",
      "Epoch 152/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 0.0014 - val_accuracy: 0.7143 - val_loss: 1.8913\n",
      "Epoch 153/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 0.0014 - val_accuracy: 0.7143 - val_loss: 1.8936\n",
      "Epoch 154/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 0.0013 - val_accuracy: 0.7143 - val_loss: 1.8959\n",
      "Epoch 155/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 1.0000 - loss: 0.0013 - val_accuracy: 0.7143 - val_loss: 1.8982\n",
      "Epoch 156/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 0.0013 - val_accuracy: 0.7143 - val_loss: 1.9006\n",
      "Epoch 157/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 0.0013 - val_accuracy: 0.7143 - val_loss: 1.9030\n",
      "Epoch 158/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 1.0000 - loss: 0.0013 - val_accuracy: 0.7143 - val_loss: 1.9054\n",
      "Epoch 159/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 1.0000 - loss: 0.0013 - val_accuracy: 0.7143 - val_loss: 1.9079\n",
      "Epoch 160/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 0.0013 - val_accuracy: 0.7143 - val_loss: 1.9102\n",
      "Epoch 161/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 0.0013 - val_accuracy: 0.7143 - val_loss: 1.9126\n",
      "Epoch 162/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 0.0013 - val_accuracy: 0.7143 - val_loss: 1.9149\n",
      "Epoch 163/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 0.0013 - val_accuracy: 0.7143 - val_loss: 1.9171\n",
      "Epoch 164/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 0.0013 - val_accuracy: 0.7143 - val_loss: 1.9194\n",
      "Epoch 165/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 1.0000 - loss: 0.0013 - val_accuracy: 0.7143 - val_loss: 1.9216\n",
      "Epoch 166/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 0.0013 - val_accuracy: 0.7143 - val_loss: 1.9239\n",
      "Epoch 167/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 0.0012 - val_accuracy: 0.7143 - val_loss: 1.9262\n",
      "Epoch 168/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 1.0000 - loss: 0.0012 - val_accuracy: 0.7143 - val_loss: 1.9285\n",
      "Epoch 169/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 0.0012 - val_accuracy: 0.7143 - val_loss: 1.9309\n",
      "Epoch 170/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 0.0012 - val_accuracy: 0.7143 - val_loss: 1.9332\n",
      "Epoch 171/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 0.0012 - val_accuracy: 0.7143 - val_loss: 1.9355\n",
      "Epoch 172/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 0.0012 - val_accuracy: 0.7143 - val_loss: 1.9379\n",
      "Epoch 173/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 1.0000 - loss: 0.0012 - val_accuracy: 0.7143 - val_loss: 1.9415\n",
      "Epoch 174/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 0.0012 - val_accuracy: 0.5714 - val_loss: 2.0675\n",
      "Epoch 175/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 0.0029 - val_accuracy: 0.7143 - val_loss: 1.9596\n",
      "Epoch 176/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 0.0020 - val_accuracy: 0.7143 - val_loss: 1.9540\n",
      "Epoch 177/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 0.0031 - val_accuracy: 0.7143 - val_loss: 1.8948\n",
      "Epoch 178/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 1.0000 - loss: 0.0013 - val_accuracy: 0.7143 - val_loss: 1.8560\n",
      "Epoch 179/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 0.0013 - val_accuracy: 0.7143 - val_loss: 1.8575\n",
      "Epoch 180/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 0.0022 - val_accuracy: 0.7143 - val_loss: 1.9054\n",
      "Epoch 181/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 1.0000 - loss: 0.0023 - val_accuracy: 0.7143 - val_loss: 1.9755\n",
      "Epoch 182/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 0.0014 - val_accuracy: 0.7143 - val_loss: 2.0373\n",
      "Epoch 183/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 0.0012 - val_accuracy: 0.7143 - val_loss: 2.0684\n",
      "Epoch 184/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 1.0000 - loss: 0.0014 - val_accuracy: 0.7143 - val_loss: 2.0547\n",
      "Epoch 185/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 0.0018 - val_accuracy: 0.7143 - val_loss: 1.9943\n",
      "Epoch 186/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 0.0016 - val_accuracy: 0.7143 - val_loss: 1.9127\n",
      "Epoch 187/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 0.0012 - val_accuracy: 0.7143 - val_loss: 1.8431\n",
      "Epoch 188/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 0.0012 - val_accuracy: 0.7143 - val_loss: 1.8123\n",
      "Epoch 189/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 0.0013 - val_accuracy: 0.7143 - val_loss: 1.8351\n",
      "Epoch 190/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 1.0000 - loss: 0.0014 - val_accuracy: 0.7143 - val_loss: 1.9032\n",
      "Epoch 191/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 0.0014 - val_accuracy: 0.7143 - val_loss: 1.9902\n",
      "Epoch 192/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 0.0012 - val_accuracy: 0.7143 - val_loss: 2.0685\n",
      "Epoch 193/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 0.0012 - val_accuracy: 0.7143 - val_loss: 2.1165\n",
      "Epoch 194/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 1.0000 - loss: 0.0012 - val_accuracy: 0.7143 - val_loss: 2.1217\n",
      "Epoch 195/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 0.0012 - val_accuracy: 0.7143 - val_loss: 2.0856\n",
      "Epoch 196/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 1.0000 - loss: 0.0012 - val_accuracy: 0.7143 - val_loss: 2.0225\n",
      "Epoch 197/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 0.0011 - val_accuracy: 0.7143 - val_loss: 1.9512\n",
      "Epoch 198/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 0.0011 - val_accuracy: 0.7143 - val_loss: 1.8906\n",
      "Epoch 199/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 0.0011 - val_accuracy: 0.7143 - val_loss: 1.8580\n",
      "Epoch 200/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 0.0011 - val_accuracy: 0.7143 - val_loss: 1.8637\n",
      "Epoch 201/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 0.0011 - val_accuracy: 0.7143 - val_loss: 1.9056\n",
      "Epoch 202/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 0.0011 - val_accuracy: 0.7143 - val_loss: 1.9699\n",
      "Epoch 203/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 1.0000 - loss: 0.0010 - val_accuracy: 0.7143 - val_loss: 2.0387\n",
      "Epoch 204/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 1.0000 - loss: 0.0010 - val_accuracy: 0.7143 - val_loss: 2.0950\n",
      "Epoch 205/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 1.0000 - loss: 0.0010 - val_accuracy: 0.7143 - val_loss: 2.1256\n",
      "Epoch 206/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 0.0011 - val_accuracy: 0.7143 - val_loss: 2.1246\n",
      "Epoch 207/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 0.0010 - val_accuracy: 0.7143 - val_loss: 2.0958\n",
      "Epoch 208/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 0.0010 - val_accuracy: 0.7143 - val_loss: 2.0500\n",
      "Epoch 209/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 1.0000 - loss: 9.8302e-04 - val_accuracy: 0.7143 - val_loss: 2.0005\n",
      "Epoch 210/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 9.7318e-04 - val_accuracy: 0.7143 - val_loss: 1.9596\n",
      "Epoch 211/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 1.0000 - loss: 9.8259e-04 - val_accuracy: 0.7143 - val_loss: 1.9365\n",
      "Epoch 212/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 9.9454e-04 - val_accuracy: 0.7143 - val_loss: 1.9354\n",
      "Epoch 213/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 1.0000 - loss: 9.9311e-04 - val_accuracy: 0.7143 - val_loss: 1.9546\n",
      "Epoch 214/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 9.7736e-04 - val_accuracy: 0.7143 - val_loss: 1.9874\n",
      "Epoch 215/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 9.5890e-04 - val_accuracy: 0.7143 - val_loss: 2.0247\n",
      "Epoch 216/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 1.0000 - loss: 9.4821e-04 - val_accuracy: 0.7143 - val_loss: 2.0579\n",
      "Epoch 217/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 1.0000 - loss: 9.4661e-04 - val_accuracy: 0.7143 - val_loss: 2.0799\n",
      "Epoch 218/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 9.4803e-04 - val_accuracy: 0.7143 - val_loss: 2.0872\n",
      "Epoch 219/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 9.4599e-04 - val_accuracy: 0.7143 - val_loss: 2.0802\n",
      "Epoch 220/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 9.3914e-04 - val_accuracy: 0.7143 - val_loss: 2.0629\n",
      "Epoch 221/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 1.0000 - loss: 9.3092e-04 - val_accuracy: 0.7143 - val_loss: 2.0413\n",
      "Epoch 222/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 9.2485e-04 - val_accuracy: 0.7143 - val_loss: 2.0217\n",
      "Epoch 223/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 9.2123e-04 - val_accuracy: 0.7143 - val_loss: 2.0091\n",
      "Epoch 224/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 9.1783e-04 - val_accuracy: 0.7143 - val_loss: 2.0060\n",
      "Epoch 225/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 9.1278e-04 - val_accuracy: 0.7143 - val_loss: 2.0120\n",
      "Epoch 226/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 1.0000 - loss: 9.0660e-04 - val_accuracy: 0.7143 - val_loss: 2.0246\n",
      "Epoch 227/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 9.0133e-04 - val_accuracy: 0.7143 - val_loss: 2.0399\n",
      "Epoch 228/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 1.0000 - loss: 8.9804e-04 - val_accuracy: 0.7143 - val_loss: 2.0538\n",
      "Epoch 229/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 1.0000 - loss: 8.9574e-04 - val_accuracy: 0.7143 - val_loss: 2.0634\n",
      "Epoch 230/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 8.9248e-04 - val_accuracy: 0.7143 - val_loss: 2.0672\n",
      "Epoch 231/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 8.8737e-04 - val_accuracy: 0.7143 - val_loss: 2.0656\n",
      "Epoch 232/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 8.8133e-04 - val_accuracy: 0.7143 - val_loss: 2.0602\n",
      "Epoch 233/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 8.7608e-04 - val_accuracy: 0.7143 - val_loss: 2.0537\n",
      "Epoch 234/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 1.0000 - loss: 8.7252e-04 - val_accuracy: 0.7143 - val_loss: 2.0485\n",
      "Epoch 235/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 1.0000 - loss: 8.7010e-04 - val_accuracy: 0.7143 - val_loss: 2.0466\n",
      "Epoch 236/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 8.6743e-04 - val_accuracy: 0.7143 - val_loss: 2.0487\n",
      "Epoch 237/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 8.6359e-04 - val_accuracy: 0.7143 - val_loss: 2.0542\n",
      "Epoch 238/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 8.5879e-04 - val_accuracy: 0.7143 - val_loss: 2.0619\n",
      "Epoch 239/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 1.0000 - loss: 8.5402e-04 - val_accuracy: 0.7143 - val_loss: 2.0696\n",
      "Epoch 240/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 8.5002e-04 - val_accuracy: 0.7143 - val_loss: 2.0757\n",
      "Epoch 241/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 8.4681e-04 - val_accuracy: 0.7143 - val_loss: 2.0789\n",
      "Epoch 242/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 1.0000 - loss: 8.4379e-04 - val_accuracy: 0.7143 - val_loss: 2.0790\n",
      "Epoch 243/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 1.0000 - loss: 8.4043e-04 - val_accuracy: 0.7143 - val_loss: 2.0766\n",
      "Epoch 244/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 8.3663e-04 - val_accuracy: 0.7143 - val_loss: 2.0729\n",
      "Epoch 245/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 8.3273e-04 - val_accuracy: 0.7143 - val_loss: 2.0695\n",
      "Epoch 246/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 1.0000 - loss: 8.2904e-04 - val_accuracy: 0.7143 - val_loss: 2.0678\n",
      "Epoch 247/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 8.2561e-04 - val_accuracy: 0.7143 - val_loss: 2.0687\n",
      "Epoch 248/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 1.0000 - loss: 8.2226e-04 - val_accuracy: 0.7143 - val_loss: 2.0724\n",
      "Epoch 249/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 8.1882e-04 - val_accuracy: 0.7143 - val_loss: 2.0782\n",
      "Epoch 250/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 1.0000 - loss: 8.1530e-04 - val_accuracy: 0.7143 - val_loss: 2.0851\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x15a6b153450>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    x = X_train,\n",
    "    y = y_train,\n",
    "    epochs = 250,\n",
    "    batch_size = 32,\n",
    "    validation_data = (X_test, y_test),\n",
    "    verbose = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1]])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = (model.predict(X_test) > 0.5).astype(int)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
